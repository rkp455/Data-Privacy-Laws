General Data Protection Regulation

Article
Talk
Read
Edit
View history

Tools
From Wikipedia, the free encyclopedia
"GDPR" redirects here. For the economics term, see Gross domestic product of region.
Regulation (EU) 2016/679
European Union regulation
Text with EEA relevance

Title	Regulation on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (Data Protection Directive)
Made by	European Parliament and Council of the European Union
Journal reference	L119, 4 May 2016, p. 1–88
History
Date made	14 April 2016
Implementation date	25 May 2018
Preparative texts
Commission proposal	COM/2012/010 final – 2012/0010 (COD)
Other legislation
Replaces	Data Protection Directive
Current legislation
The General Data Protection Regulation (Regulation (EU) 2016/679, abbreviated GDPR) is a European Union regulation on information privacy in the European Union (EU) and the European Economic Area (EEA). The GDPR is an important component of EU privacy law and human rights law, in particular Article 8(1) of the Charter of Fundamental Rights of the European Union. It also governs the transfer of personal data outside the EU and EEA. The GDPR's goals are to enhance individuals' control and rights over their personal information and to simplify the regulations for international business.[1] It supersedes the Data Protection Directive 95/46/EC and, among other things, simplifies the terminology.

The European Parliament and Council of the European Union adopted the GDPR on 14 April 2016, to become effective on 25 May 2018. As an EU regulation (instead of a directive), GDPR is directly applicable with force of law on its own without the need of transposition. However, it also provides flexibility for individual member states to modify (derogate from) some of its provisions.

The regulation became a model for many other laws around the world, including in Turkey, Mauritius, Chile, Japan, Brazil, South Korea, South Africa, Argentina and Kenya. After leaving the European Union the United Kingdom enacted its "UK GDPR", identical to the GDPR.[2] The California Consumer Privacy Act (CCPA), adopted on 28 June 2018, has many similarities with the GDPR.[3]

Contents
The GDPR 2016 has eleven chapters, concerning general provisions, principles, rights of the data subject, duties of data controllers or processors, transfers of personal data to third countries, supervisory authorities, cooperation among member states, remedies, liability or penalties for breach of rights, and miscellaneous final provisions. Recital 4 proclaims that ‘processing of personal data should be designed to serve mankind’.

General provisions
The regulation applies if the data controller (an organisation that collects information about living people, whether they are in the EU or not), or processor (an organisation that processes data on behalf of a data controller like cloud service providers), or the data subject (person) is based in the EU. Under certain circumstances,[4] the regulation also applies to organisations based outside the EU if they collect or process personal data of individuals located inside the EU. The regulation does not apply to the processing of data by a person for a "purely personal or household activity and thus with no connection to a professional or commercial activity." (Recital 18).

According to the European Commission, "Personal data is information that relates to an identified or identifiable individual. If you cannot directly identify an individual from that information, then you need to consider whether the individual is still identifiable. You should take into account the information you are processing together with all the means reasonably likely to be used by either you or any other person to identify that individual."[5] The precise definitions of terms such as "personal data", "processing", "data subject", "controller", and "processor" are stated in Article 4 of the Regulation.[6]

The regulation does not purport to apply to the processing of personal data for national security activities or law enforcement of the EU; however, industry groups concerned about facing a potential conflict of laws have questioned whether Article 48[6] of the GDPR could be invoked to seek to prevent a data controller subject to a third country's laws from complying with a legal order from that country's law enforcement, judicial, or national security authorities to disclose to such authorities the personal data of an EU person, regardless of whether the data resides in or out of the EU. Article 48 states that any judgement of a court or tribunal and any decision of an administrative authority of a third country requiring a controller or processor to transfer or disclose personal data may not be recognised or enforceable in any manner unless based on an international agreement, like a mutual legal assistance treaty in force between the requesting third (non-EU) country and the EU or a member state.[7] The data protection reform package also includes a separate Data Protection Directive for the police and criminal justice sector that provides rules on personal data exchanges at State level, Union level, and international levels.[8]

A single set of rules applies to all EU member states. Each member state establishes an independent supervisory authority (SA) to hear and investigate complaints, sanction administrative offences, etc. SAs in each member state co-operate with other SAs, providing mutual assistance and organising joint operations. If a business has multiple establishments in the EU, it must have a single SA as its "lead authority", based on the location of its "main establishment" where the main processing activities take place. The lead authority thus acts as a "one-stop shop" to supervise all the processing activities of that business throughout the EU (Articles 46–55 of the GDPR).[9][10] A European Data Protection Board (EDPB) co-ordinates the SAs. EDPB thus replaces the Article 29 Data Protection Working Party. There are exceptions for data processed in an employment context or in national security that still might be subject to individual country regulations (Articles 2(2)(a) and 88 of the GDPR).

Principles and lawful purposes
Article 5 sets out six principles relating to the lawfulness of processing personal data. The first of these specifies that data must be processed lawfully, fairly and in a transparent manner. Article 6 develops this principle by specifying that personal data may not be processed unless there is at least one legal basis for doing so. The other principles refer to "purpose limitation", "data minimisation", "accuracy", "storage limitation", and "integrity and confidentiality".[11]: Article 5 

Article 6 states that the lawful purposes are:[11]: Article 6 

(a) If the data subject has given consent to the processing of his or her personal data;
(b) To fulfill contractual obligations with a data subject, or for tasks at the request of a data subject who is in the process of entering into a contract;
(c) To comply with a data controller's legal obligations;
(d) To protect the vital interests of a data subject or another individual;
(e) To perform a task in the public interest or in official authority;
(f) For the legitimate interests of a data controller or a third party, unless these interests are overridden by interests of the data subject or her or his rights according to the Charter of Fundamental Rights (especially in the case of children).[7]
If informed consent is used as the lawful basis for processing, consent must have been explicit for data collected and each purpose data is used for (Article 7; defined in Article 4).[12][13] Consent must be a specific, freely given, plainly worded, and unambiguous affirmation given by the data subject; an online form which has consent options structured as an opt-out selected by default is a violation of the GDPR, as the consent is not unambiguously affirmed by the user. In addition, multiple types of processing may not be "bundled" together into a single affirmation prompt, as this is not specific to each use of data, and the individual permissions are not freely given. (Recital 32).

Data subjects must be allowed to withdraw this consent at any time, and the process of doing so must not be harder than it was to opt in. (Article 7(3)) A data controller may not refuse service to users who decline consent to processing that is not strictly necessary in order to use the service. (Article 8) Consent for children, defined in the regulation as being less than 16 years old (although with the option for member states to individually make it as low as 13 years old (Article 8(1)), must be given by the child's parent or custodian, and verifiable (Article 8).[14][15]

If consent to processing was already provided under the Data Protection Directive, a data controller does not have to re-obtain consent if the processing is documented and obtained in compliance with the GDPR's requirements (Recital 171).[16][17]

Rights of the data subject
Transparency and modalities
Article 12 requires the data controller to provide information to the "data subject in a concise, transparent, intelligible and easily accessible form, using clear and plain language, in particular for any information addressed specifically to a child."[7]

Information and access
The right of access (Article 15) is a data subject right.[18] It gives people the right to access their personal data and information about how this personal data is being processed. A data controller must provide, upon request, an overview of the categories of data that are being processed (Article 15(1)(b)) as well as a copy of the actual data (Article 15(3)); furthermore, the data controller has to inform the data subject on details about the processing, such as the purposes of the processing (Article 15(1)(a)), with whom the data is shared (Article 15(1)(c)), and how it acquired the data (Article 15(1)(g)).

A data subject must be able to transfer personal data from one electronic processing system to and into another, without being prevented from doing so by the data controller. Data that has been sufficiently anonymised is excluded, but data that has been only de-identified but remains possible to link to the individual in question, such as by providing the relevant identifier, is not.[19] In practice, however, providing such identifiers can be challenging, such as in the case of Apple's Siri, where voice and transcript data is stored with a personal identifier that the manufacturer restricts access to,[20] or in online behavioural targeting, which relies heavily on device fingerprints that can be challenging to capture, send, and verify.[21]

Both data being 'provided' by the data subject and data being 'observed', such as about behaviour, are included. In addition, the data must be provided by the controller in a structured and commonly used standard electronic format. The right to data portability is provided by Article 20 of the GDPR.[22]

Rectification and erasure
A right to be forgotten was replaced by a more limited right of erasure in the version of the GDPR that was adopted by the European Parliament in March 2014.[23][24] Article 17 provides that the data subject has the right to request erasure of personal data related to them on any one of a number of grounds, including noncompliance with Article 6(1) (lawfulness) that includes a case (f) if the legitimate interests of the controller are overridden by the interests or fundamental rights and freedoms of the data subject, which require protection of personal data (see also Google Spain SL, Google Inc. v Agencia Española de Protección de Datos, Mario Costeja González).[7][25]

Right to object and automated decisions
Article 21 of the GDPR allows an individual to object to processing personal information for marketing or non-service related purposes.[26] This means the data controller must allow an individual the right to stop or prevent controller from processing their personal data.

There are some instances where this objection does not apply. For example, if:

Legal or official authority is being carried out
"Legitimate interest", where the organisation needs to process data in order to provide the data subject with a service they signed up for
A task being carried out for public interest.
GDPR is also clear that the data controller must inform individuals of their right to object from the first communication the controller has with them. This should be clear and separate from any other information the controller is providing and give them their options for how best to object to the processing of their data.

There are instances the controller can refuse a request, in the circumstances that the objection request is "manifestly unfounded" or "excessive", so each case of objection must be looked at individually.[26] Other countries such as Canada [27] are also, following the GDPR, considering legislation to regulate automated decision making under privacy laws, even though there are policy questions as to whether this is the best way to regulate AI.[citation needed]

Right to compensation
Article 82 of the GDPR stipulates that any person who has suffered material or non-material damage as a result of an infringement of this Regulation shall have the right to receive compensation from the controller or processor for the damage suffered.

In the judgment Österreichische Post (C-300/21) the Court of Justice of the European Union gave an interpretation of the right to compensation.[28] Article 82(1) GDPR requires for the award of damages (i) an infringement of the GDPR, (ii) (actual) damage suffered and (iii) a causal link between the infringement and the damage suffered. It is not necessary that the damage suffered reaches a certain degree of seriousness. There is no European defined concept of damage. Compensation is determined nationally in accordance with national law. The principles of equivalence and effectiveness must be taken into account.[29]

See also the Opinion of the Advocate General in the case Krankenversicherung Nordrhein (C-667/21).[30]

Controller and processor
Data controllers must clearly disclose any data collection, declare the lawful basis and purpose for data processing, and state how long data is being retained and if it is being shared with any third parties or outside of the EEA. Firms have the obligation to protect data of employees and consumers to the degree where only the necessary data is extracted with minimum interference with data privacy from employees, consumers, or third parties. Firms should have internal controls and regulations for various departments such as audit, internal controls, and operations. Data subjects have the right to request a portable copy of the data collected by a controller in a common format, as well as the right to have their data erased under certain circumstances. Public authorities, and businesses whose core activities consist of regular or systematic processing of personal data, are required to employ a data protection officer (DPO), who is responsible for managing compliance with the GDPR. Data controllers must report data breaches to national supervisory authorities within 72 hours if they have an adverse effect on user privacy. In some cases, violators of the GDPR may be fined up to €20 million or up to 4% of the annual worldwide turnover of the preceding financial year in case of an enterprise, whichever is greater.

To be able to demonstrate compliance with the GDPR, the data controller must implement measures that meet the principles of data protection by design and by default. Article 25 requires data protection measures to be designed into the development of business processes for products and services. Such measures include pseudonymising personal data, by the controller, as soon as possible (Recital 78). It is the responsibility and the liability of the data controller to implement effective measures and be able to demonstrate the compliance of processing activities even if the processing is carried out by a data processor on behalf of the controller (Recital 74).[7] When data is collected, data subjects must be clearly informed about the extent of data collection, the legal basis for the processing of personal data, how long data is retained, if data is being transferred to a third-party and/or outside the EU, and any automated decision-making that is made on a solely algorithmic basis. Data subjects must be informed of their privacy rights under the GDPR, including their right to revoke consent to data processing at any time, their right to view their personal data and access an overview of how it is being processed, their right to obtain a portable copy of the stored data, their right to erasure of their data under certain circumstances, their right to contest any automated decision-making that was made on a solely algorithmic basis, and their right to file complaints with a Data Protection Authority. As such, the data subject must also be provided with contact details for the data controller and their designated data protection officer, where applicable.[31][32]

Data protection impact assessments (Article 35) have to be conducted when specific risks occur to the rights and freedoms of data subjects. Risk assessment and mitigation is required and prior approval of the data protection authorities is required for high risks.

Article 25 requires data protection to be designed into the development of business processes for products and services. Privacy settings must therefore be set at a high level by default, and technical and procedural measures shall be taken by the controller to make sure that the processing, throughout the whole processing lifecycle, complies with the regulation. Controllers shall also implement mechanisms to ensure that personal data is not processed unless necessary for each specific purpose. This is known as data minimisation.

A report[33] by the European Union Agency for Network and Information Security elaborates on what needs to be done to achieve privacy and data protection by default. It specifies that encryption and decryption operations must be carried out locally, not by remote service, because both keys and data must remain in the power of the data owner if any privacy is to be achieved. The report specifies that outsourced data storage on remote clouds is practical and relatively safe if only the data owner, not the cloud service, holds the decryption keys.

Pseudonymisation
According to the GDPR, pseudonymisation is a required process for stored data that transforms personal data in such a way that the resulting data cannot be attributed to a specific data subject without the use of additional information (as an alternative to the other option of complete data anonymisation).[34] An example is encryption, which renders the original data unintelligible in a process that cannot be reversed without access to the correct decryption key. The GDPR requires for the additional information (such as the decryption key) to be kept separately from the pseudonymised data.

Another example of pseudonymisation is tokenisation, which is a non-mathematical approach to protecting data at rest that replaces sensitive data with non-sensitive substitutes, referred to as tokens. While the tokens have no extrinsic or exploitable meaning or value, they allow for specific data to be fully or partially visible for processing and analytics while sensitive information is kept hidden. Tokenisation does not alter the type or length of data, which means it can be processed by legacy systems such as databases that may be sensitive to data length and type. This also requires much fewer computational resources to process and less storage space in databases than traditionally encrypted data.

Pseudonymisation is a privacy-enhancing technology and is recommended to reduce the risks to the concerned data subjects and also to help controllers and processors to meet their data protection obligations (Recital 28).[35]

Records of processing activities
According to Article 30,[7] records of processing activities have to be maintained by each organisation matching one of following criteria:

employing more than 250 people;
the processing it carries out is likely to result in a risk to the rights and freedoms of data subjects;
the processing is not occasional;
processing includes special categories of data as referred to in Article 9(1) or personal data relating to criminal convictions and offences referred to in Article 10.
Such requirements may be modified by each EU country. The records shall be in electronic form and the controller or the processor and, where applicable, the controller's or the processor's representative, shall make the record available to the supervisory authority on request.

Records of controller shall contain all of the following information:

the name and contact details of the controller and, where applicable, the joint controller,[a] the controller's representative and the data protection officer;
the purposes of the processing;
a description of the categories of data subjects and of the categories of personal data;
the categories of recipients to whom the personal data have been or will be disclosed including recipients in third countries or international organisations;
where applicable, transfers of personal data to a third country or an international organisation, including the identification of that third country or international organisation and, in the case of transfers referred to in the second subparagraph of Article 49(1), the documentation of suitable safeguards;
where possible, the envisaged time limits for erasure of the different categories of data;
where possible, a general description of the technical and organisational security measures referred to in Article 32(1).
Records of processor shall contain all of the following information:

the name and contact details of the processor or processors and of each controller on behalf of which the processor is acting, and, where applicable, of the controller's or the processor's representative, and the data protection officer;
the categories of processing carried out on behalf of each controller;
where applicable, transfers of personal data to a third country or an international organisation, including the identification of that third country or international organisation and, in the case of transfers referred to in the second subparagraph of Article 49(1), the
documentation of suitable safeguards;
where possible, a general description of the technical and organisational security measures referred to in Article 32(1).[7]
Security of personal data
Controllers and processors of personal data must put in place appropriate technical and organizational measures to implement the data protection principles.[36] Business processes that handle personal data must be designed and built with consideration of the principles and provide safeguards to protect data (for example, using pseudonymization or full anonymization where appropriate).[37] Data controllers must design information systems with privacy in mind. For instance, using the highest-possible privacy settings by default, so that the datasets are not publicly available by default and cannot be used to identify a subject.[38] No personal data may be processed unless this processing is done under one of the six lawful bases specified by the regulation (consent, contract, public task, vital interest, legitimate interest or legal requirement). When the processing is based on consent the data subject has the right to revoke it at any time.[39]

Article 33 states the data controller is under a legal obligation to notify the supervisory authority without undue delay unless the breach is unlikely to result in a risk to the rights and freedoms of the individuals. There is a maximum of 72 hours after becoming aware of the data breach to make the report. Individuals have to be notified if a high risk of an adverse impact is determined (Article 34). In addition, the data processor will have to notify the controller without undue delay after becoming aware of a personal data breach (Article 33). However, the notice to data subjects is not required if the data controller has implemented appropriate technical and organisational protection measures that render the personal data unintelligible to any person who is not authorised to access it, such as encryption (Article 34).[7]

Data protection officer
See also: European Commission Data Protection Officer
Article 37 requires appointment of a data protection officer. If processing is carried out by a public authority (except for courts or independent judicial authorities when acting in their judicial capacity), or if processing operations involve regular and systematic monitoring of data subjects on a large scale, or if processing on a large scale of special categories of data and personal data relating to criminal convictions and offences (Articles 9 and Article 10,[40]) a data protection officer (DPO)—a person with expert knowledge of data protection law and practices—must be designated to assist the controller or processor in monitoring their internal compliance with the Regulation.[7]

A designated DPO can be a current member of staff of a controller or processor, or the role can be outsourced to an external person or agency through a service contract. In any case, the processing body must make sure that there is no conflict of interest in other roles or interests that a DPO may hold. The contact details for the DPO must be published by the processing organisation (for example, in a privacy notice) and registered with the supervisory authority.

The DPO is similar to a compliance officer and is also expected to be proficient at managing IT processes, data security (including dealing with cyberattacks) and other critical business continuity issues associated with the holding and processing of personal and sensitive data. The skill set required stretches beyond understanding legal compliance with data protection laws and regulations. The DPO must maintain a living data inventory of all data collected and stored on behalf of the organization.[41] More details on the function and the role of data protection officer were given on 13 December 2016 (revised 5 April 2017) in a guideline document.[42]

Organisations based outside the EU must also appoint an EU-based person as a representative and point of contact for their GDPR obligations (Article 27). This is a distinct role from a DPO, although there is overlap in responsibilities that suggest that this role can also be held by the designated DPO.[43]

Remedies, liability and penalties
See also: GDPR fines and notices
Besides the definitions as a criminal offence according to national law following Article 83 GDPR the following sanctions can be imposed:

a warning in writing in cases of first and non-intentional noncompliance
regular periodic data protection audits
a fine up to €10 million or up to 2% of the annual worldwide turnover of the preceding financial year in case of an enterprise, whichever is greater, if there has been an infringement of the following provisions (Article 83, Paragraph 4[44]):
the obligations of the controller and the processor pursuant to Articles 8, 11, 25 to 39, and 42 and 43
the obligations of the certification body pursuant to Articles 42 and 43
the obligations of the monitoring body pursuant to Article 41(4)
a fine up to €20 million or up to 4% of the annual worldwide turnover of the preceding financial year in case of an enterprise, whichever is greater, if there has been an infringement of the following provisions (Article 83, Paragraph 5 & 6[44]):
the basic principles for processing, including conditions for consent, pursuant to Articles 5, 6, 7, and 9
the data subjects' rights pursuant to Articles 12 to 22
the transfers of personal data to a recipient in a third country or an international organisation pursuant to Articles 44 to 49
any obligations pursuant to member state law adopted under Chapter IX
noncompliance with an order or a temporary or definitive limitation on processing or the suspension of data flows by the supervisory authority pursuant to Article 58(2) or failure to provide access in violation of Article 58(1)[7]
Exemptions
These are some cases which are not addressed in the GDPR specifically, thus are treated as exemptions.[45]

Personal or household activities
Law enforcement
National security[7]
When the GDPR was being created, it was strictly created for the regulation of personal data which goes into the hands of companies.[citation needed] What is not covered by the GDPR is non-commercial information or household activities.[46][failed verification] An example of these household activities may be emails between two high school friends.

Conversely, an entity or more precisely an "enterprise" has to be engaged in "economic activity" to be covered by the GDPR.[b] Economic activity is defined broadly under European Union competition law.[47]

Applicability outside of the European Union
The GDPR also applies to data controllers and processors outside of the European Economic Area (EEA) if they are engaged in the "offering of goods or services" (regardless of whether a payment is required) to data subjects within the EEA, or are monitoring the behaviour of data subjects within the EEA (Article 3(2)). The regulation applies regardless of where the processing takes place.[48] This has been interpreted as intentionally giving GDPR extraterritorial jurisdiction for non-EU establishments if they are doing business with people located in the EU. It is questionable whether the EU or its member states will in practice be able to enforce GDPR against organisations which have no establishment in the EU.[49]

EU Representative
Under Article 27, non-EU establishments subject to GDPR are obliged to have a designee within the European Union, an "EU Representative", to serve as a point of contact for their obligations under the regulation. The EU Representative is the Controller's or Processor's contact person vis-à-vis European privacy supervisors and data subjects, in all matters relating to processing, to ensure compliance with this GDPR. A natural (individual) or legal (corporation) person can play the role of an EU Representative.[50] The non-EU establishment must issue a duly signed document (letter of accreditation) designating a given individual or company as its EU Representative. The said designation can only be given in writing.[51]

An establishment's failure to designate an EU Representative is considered ignorance of the regulation and relevant obligations, which itself is a violation of the GDPR subject to fines of up to €10 million or up to 2% of the annual worldwide turnover of the preceding financial year in case of an enterprise, whichever is greater. The intentional or negligent (willful blindness) character of the infringement (failure to designate an EU Representative) may rather constitute aggravating factors.[52]

An establishment does not need to name an EU Representative if they only engage in occasional processing that does not include, on a large scale, processing of special categories of data as referred to in Article 9(1) of GDPR or processing of personal data relating to criminal convictions and offences referred to in Article 10, and such processing is unlikely to result in a risk to the rights and freedoms of natural persons, taking into account the nature, context, scope and purposes of the processing.[7] Non-EU public authorities and bodies are equally exempted.[53]

Third countries
Chapter V of the GDPR forbids the transfer of the personal data of EU data subjects to countries outside of the EEA — known as third countries — unless appropriate safeguards are imposed, or the third country's data protection regulations are formally considered adequate by the European Commission (Article 45).[54][55] Binding corporate rules, standard contractual clauses for data protection issued by a Data Processing Agreement (DPA), or a scheme of binding and enforceable commitments by the data controller or processor situated in a third country, are among examples.[56]

United Kingdom implementation
Explanation of the possible results from UK's divergence from the European GDPR[57]
The applicability of GDPR in the United Kingdom is affected by Brexit. Although the United Kingdom formally withdrew from the European Union on 31 January 2020, it remained subject to EU law, including GDPR, until the end of the transition period on 31 December 2020.[54] The United Kingdom granted royal assent to the Data Protection Act 2018 on 23 May 2018, which augmented the GDPR, including aspects of the regulation that are to be determined by national law, and criminal offences for knowingly or recklessly obtaining, redistributing, or retaining personal data without the consent of the data controller.[58][59]

Under the European Union (Withdrawal) Act 2018, existing and relevant EU law was transposed into local law upon completion of the transition, and the GDPR was amended by statutory instrument to remove certain provisions no longer needed due to the UK's non-membership in the EU. Thereafter, the regulation will be referred to as "UK GDPR".[60][55][54] The UK will not restrict the transfer of personal data to countries within the EEA under UK GDPR. However, the UK will become a third country under the EU GDPR, meaning that personal data may not be transferred to the country unless appropriate safeguards are imposed, or the European Commission performs an adequacy decision on the suitability of British data protection legislation (Chapter V). As part of the withdrawal agreement, the European Commission committed to perform an adequacy assessment.[54][55]

In April 2019, the UK Information Commissioner's Office (ICO) issued a children's code of practice for social networking services when used by minors, enforceable under GDPR, which also includes restrictions on "like" and "streak" mechanisms in order to discourage social media addiction and on the use of this data for processing interests.[61][62]

In March 2021, Secretary of State for Digital, Culture, Media and Sport Oliver Dowden stated that the UK was exploring divergence from the EU GDPR in order to "[focus] more on the outcomes that we want to have and less on the burdens of the rules imposed on individual businesses".[63]

Misconceptions
Some common misconceptions about GDPR include:

All processing of personal data requires consent of the data subject
In fact, data can be processed without consent if one of the other five lawful bases for processing applies, and obtaining consent may often be inappropriate.[64]
Individuals have an absolute right to have their data deleted (right to be forgotten)
Whilst there is an absolute right to opt-out of direct marketing, data controllers can continue to process personal data where they have a lawful basis to do so, as long as the data remain necessary for the purpose for which it was originally collected.[65]
Removing individuals' names from records takes them out of scope of GDPR
"Pseudonymous" data where an individual is identified by a number can still be personal data if the data controller is capable of tying that data back to an individual in another way.[66]
GDPR applies to anyone processing personal data of EU citizens anywhere in the world
In fact, it applies to non-EU established organizations only where they are processing data of data subjects located in the EU (irrespective of their citizenship) and then only when supplying goods or services to them, or monitoring their behaviour.[67]
Reception
As per a study conducted by Deloitte in 2018, 92% of companies believe they are able to comply with GDPR in their business practices in the long run.[68]

Companies operating outside of the EU have invested heavily to align their business practices with GDPR. The area of GDPR consent has a number of implications for businesses who record calls as a matter of practice. A typical disclaimer is not considered sufficient to gain assumed consent to record calls. Additionally, when recording has commenced, should the caller withdraw their consent, then the agent receiving the call must be able to stop a previously started recording and ensure the recording does not get stored.[69]

IT professionals expect that compliance with the GDPR will require additional investment overall: over 80 percent of those surveyed expected GDPR-related spending to be at least US$100,000.[70] The concerns were echoed in a report commissioned by the law firm Baker & McKenzie that found that "around 70 percent of respondents believe that organizations will need to invest additional budget/effort to comply with the consent, data mapping and cross-border data transfer requirements under the GDPR."[71] The total cost for EU companies is estimated at €200 billion while for US companies the estimate is for $41.7 billion.[72] It has been argued that smaller businesses and startup companies might not have the financial resources to adequately comply with the GDPR, unlike the larger international technology firms (such as Facebook and Google) that the regulation is ostensibly meant to target first and foremost.[73][74] A lack of knowledge and understanding of the regulations has also been a concern in the lead-up to its adoption.[75] A counter-argument to this has been that companies were made aware of these changes two years prior to them coming into effect and should have had enough time to prepare.[76]

The regulations, including whether an enterprise must have a data protection officer, have been criticized for potential administrative burden and unclear compliance requirements.[77] Although data minimisation is a requirement, with pseudonymisation being one of the possible means, the regulation provides no guidance on how or what constitutes an effective data de-identification scheme, with a grey area on what would be considered as inadequate pseudonymisation subject to Section 5 enforcement actions.[35][78][79] There is also concern regarding the implementation of the GDPR in blockchain systems, as the transparent and fixed record of blockchain transactions contradicts the very nature of the GDPR.[80] Many media outlets have commented on the introduction of a "right to explanation" of algorithmic decisions,[81][82] but legal scholars have since argued that the existence of such a right is highly unclear without judicial tests and is limited at best.[83][84]

The GDPR has garnered support from businesses who regard it as an opportunity to improve their data management.[85][86] Mark Zuckerberg has also called it a "very positive step for the Internet",[87] and has called for GDPR-style laws to be adopted in the US.[88] Consumer rights groups such as The European Consumer Organisation are among the most vocal proponents of the legislation.[89] Other supporters have attributed its passage to the whistleblower Edward Snowden.[90] Free software advocate Richard Stallman has praised some aspects of the GDPR but called for additional safeguards to prevent technology companies from "manufacturing consent".[91]

Impact
Academic experts who participated in the formulation of the GDPR wrote that the law "is the most consequential regulatory development in information policy in a generation. The GDPR brings personal data into a complex and protective regulatory regime."[92]

Despite having had at least two years to prepare and do so, many companies and websites changed their privacy policies and features worldwide directly prior to GDPR's implementation, and customarily provided email and other notifications discussing these changes. This was criticised for resulting in a fatiguing number of communications, while experts noted that some reminder emails incorrectly asserted that new consent for data processing had to be obtained for when the GDPR took effect (any previously obtained consent to processing is valid as long as it met the regulation's requirements). Phishing scams also emerged using falsified versions of GDPR-related emails, and it was also argued that some GDPR notice emails may have actually been sent in violation of anti-spam laws.[93][16] In March 2019, a provider of compliance software found that many websites operated by EU member state governments contained embedded tracking from ad technology providers.[94][95]

The deluge of GDPR-related notices also inspired memes, including those surrounding privacy policy notices being delivered by atypical means (such as a Ouija board or Star Wars opening crawl), suggesting that Santa Claus's "naughty or nice" list was a violation, and a recording of excerpts from the regulation by a former BBC Radio 4 Shipping Forecast announcer. A blog, GDPR Hall of Shame, was also created to showcase unusual delivery of GDPR notices, and attempts at compliance that contained egregious violations of the regulation's requirements. Its author remarked that the regulation "has a lot of nitty gritty, in-the-weeds details, but not a lot of information about how to comply", but also acknowledged that businesses had two years to comply, making some of its responses unjustified.[96][97][98][99][100]

Research indicates that approximately 25% of software vulnerabilities have GDPR implications.[101] Since Article 33 emphasizes breaches, not bugs, security experts advise companies to invest in processes and capabilities to identify vulnerabilities before they can be exploited, including coordinated vulnerability disclosure processes.[102][103] An investigation of Android apps' privacy policies, data access capabilities, and data access behaviour has shown that numerous apps display a somewhat privacy-friendlier behaviour since the GDPR was implemented, although they still retain most of their data access privileges in their code.[104][105] An investigation of the Norwegian Consumer Council into the post-GDPR data subject dashboards on social media platforms (such as Google dashboard) has concluded that large social media firms deploy deceptive tactics in order to discourage their customers from sharpening their privacy settings.[106]

On the effective date, some websites began to block visitors from EU countries entirely (including Instapaper,[107] Unroll.me,[108] and Tribune Publishing-owned newspapers, such as the Chicago Tribune and the Los Angeles Times) or redirect them to stripped-down versions of their services (in the case of National Public Radio and USA Today) with limited functionality and/or no advertising so that they will not be liable.[109][110][111][112] Some companies, such as Klout, and several online video games, ceased operations entirely to coincide with its implementation, citing the GDPR as a burden on their continued operations, especially due to the business model of the former.[113][114][115] The volume of online behavioural advertising placements in Europe fell 25–40% on 25 May 2018.[116][117]

In 2020, two years after the GDPR began its implementation, the European Commission assessed that users across the EU had increased their knowledge about their rights, stating that "69% of the population above the age of 16 in the EU have heard about the GDPR and 71% of people heard about their national data protection authority."[118][119] The commission also found that privacy has become a competitive quality for companies which consumers are taking into account in their decisionmaking processes.[118]

Enforcement and inconsistency
Main article: GDPR fines and notices
Facebook and subsidiaries WhatsApp and Instagram, as well as Google LLC (targeting Android), were immediately sued by Max Schrems's non-profit NOYB just hours after midnight on 25 May 2018, for their use of "forced consent". Schrems asserts that both companies violated Article 7(4) by not presenting opt-ins for data processing consent on an individualized basis, and requiring users to consent to all data processing activities (including those not strictly necessary) or would be forbidden from using the services.[120][121][122][123][124] On 21 January 2019, Google was fined €50 million by the French DPA for showing insufficient control, consent, and transparency over use of personal data for behavioural advertising.[125][126] In November 2018, following a journalistic investigation into Liviu Dragnea, the Romanian DPA (ANSPDCP) used a GDPR request to demand information on the RISE Project's sources.[127][128]

In July 2019, the British Information Commissioner's Office issued an intention to fine British Airways a record £183 million (1.5% of turnover) for poor security arrangements that enabled a 2018 web skimming attack affecting around 380,000 transactions.[129][130][131][132][133] British Airways was ultimately fined a reduced amount of £20m, with the ICO noting that they had "considered both representations from BA and the economic impact of COVID-19 on their business before setting a final penalty".[134]

In December 2019, Politico reported that Ireland and Luxembourg – two smaller EU countries that have had a reputation as a tax havens and (especially in the case of Ireland) as a base for European subsidiaries of U.S. big tech companies – were facing significant backlogs in their investigations of major foreign companies under GDPR, with Ireland citing the complexity of the regulation as a factor. Critics interviewed by Politico also argued that enforcement was also being hampered by varying interpretations between member states, the prioritisation of guidance over enforcement by some authorities, and a lack of cooperation between member states.[135]

In November 2021, Irish Council for Civil Liberties lodged a formal complaint of the Commission that it is in breach of its obligation under EU Law to carefully monitor how Ireland applies the GDPR.[136] Until January 2023, the Commission published a new commitment based on the complaint of ICCL.[136]

While companies are now subject to legal obligations, there are still various inconsistencies in the practical and technical implementation of GDPR.[137] As an example, according to the GDPR's right to access, the companies are obliged to provide data subjects with the data they gather about them. However, in a study on loyalty cards in Germany, companies did not provide the data subjects with the exact information of the purchased articles.[138] One might argue that such companies do not collect the information of the purchased articles, which does not conform with their business models. Therefore, data subjects tend to see that as a GDPR violation. As a result, studies have suggested for a better control through authorities.[138]

According to the GDPR, end-users' consent should be valid, freely given, specific, informed and active.[139] However, the lack of enforceability regarding obtaining lawful consents has been a challenge. As an example, a 2020 study, showed that the Big Tech, i.e. Google, Amazon, Facebook, Apple, and Microsoft (GAFAM), use dark patterns in their consent obtaining mechanisms, which raises doubts regarding the lawfulness of the acquired consent.[139]

In March 2021, EU member states led by France were reported to be attempting to modify the impact of the privacy regulation in Europe by exempting national security agencies.[140]

After around 160 million Euros in GDPR fines were imposed in 2020, the figure was already over one billion Euros in 2021.[141]

Influence on foreign laws
Mass adoption of these new privacy standards by multinational companies has been cited as an example of the "Brussels effect", a phenomenon wherein European laws and regulations are used as a baseline due to their gravitas.[142]

The U.S. state of California passed the California Consumer Privacy Act on 28 June 2018, taking effect on 1 January 2020; it grants rights to transparency and control over the collection of personal information by companies in a similar means to GDPR. Critics have argued that such laws need to be implemented at the federal level to be effective, as a collection of state-level laws would have varying standards that would complicate compliance.[143][144][145] Two other U.S. states have since enacted similar legislation: Virginia passed the Consumer Data Privacy Act on 2 March 2021,[146] and Colorado enacted the Colorado Privacy Act on 8 July 2021.[147]

The Republic of Turkey, a candidate for European Union membership, has adopted the Law on The Protection of Personal Data on 24 March 2016 in compliance with the EU acquis.[148]

China's 2021 Personal Information Protection Law is the country's first comprehensive law on personal data rights and is modeled after the GDPR.[149]: 131 

Switzerland will also adopt a new data protection law that largely follows EU's GDPR.[150]

Website views and revenue
A 2024 study found that GDPR reduced both EU user website page views and website revenue by 12%.[151]

Timeline
25 January 2012: The proposal for the GDPR was released.[10]
21 October 2013: The European Parliament Committee on Civil Liberties, Justice and Home Affairs (LIBE) had its orientation vote.
15 December 2015: Negotiations between the European Parliament, Council and Commission (Formal Trilogue meeting) resulted in a joint proposal.
17 December 2015: The European Parliament's LIBE Committee voted for the negotiations between the three parties.
8 April 2016: Adoption by the Council of the European Union.[152] The only member state voting against was Austria, which argued that the level of data protection in some respects falls short compared to the 1995 directive.[153][154]
14 April 2016: Adoption by the European Parliament.[155]
24 May 2016: The regulation entered into force, 20 days after its publication in the Official Journal of the European Union.[18]
6 May 2018: Data Protection Directive for the police and justice sectors into national legislation applicable from this day.[156]
25 May 2018: Its provisions became directly applicable in all member states, two years after the regulations enter into force.[18]
20 July 2018: the GDPR became valid in the EEA countries (Iceland, Liechtenstein, and Norway),[157] after the EEA Joint Committee and the three countries agreed to follow the regulation.[158]
EU Digital Single Market
The EU Digital Single Market strategy relates to "digital economy" activities related to businesses and people in the EU.[159] As part of the strategy, the GDPR and the NIS Directive all apply from 25 May 2018. The proposed ePrivacy Regulation was also planned to be applicable from 25 May 2018, but will be delayed for several months.[160] The eIDAS Regulation is also part of the strategy.

In an initial assessment, the European Council has stated that the GDPR should be considered "a prerequisite for the development of future digital policy initiatives".[161]

See also
flag	European Union portal
icon	Law portal
Similar privacy laws in other countries:

California Consumer Privacy Act (CCPA)
Children's Online Privacy Protection Act (COPPA) (USA)
General Personal Data Protection Law (LGPD) (Brazil)
Personal Data Protection Act 2012 (PDPA) (Singapore)
Personal Information Protection Law (PIPL) (China)
Protection of Personal Information Act (PoPIA) (South Africa)
Related EU regulation:

Data Act, proposed EU law from 2022
Data Governance Act, proposed EU law from 2020
Digital Markets Act
Digital Services Act
EU–US Privacy Shield
European Data Protection Board (EDPB)
European Health Data Space
Privacy and Electronic Communications Directive 2002 (ePrivacy Directive, ePD)
Related concepts:

Convention on Cybercrime
Data portability
Do Not Track legislation
ePrivacy Regulation
Privacy Impact Assessment
Compliance tactics by certain companies:

Consent or pay
Footnotes
 Joint control arises "where two or more controllers jointly determine the purposes and means of processing" data. They are required to agree their respective responsibilities in a "transparent" manner and to communicate "the essence of the arrangement" to data subjects.[6]: Article 26 
 Refer GDPR article 4(18): 'enterprise' means a natural or legal person engaged in an economic activity, irrespective of its legal form, including partnerships or associations regularly engaged in an economic activity.[7]
References
 "Presidency of the Council: 'Compromise text. Several partial general approaches have been instrumental in converging views in Council on the proposal for a General Data Protection Regulation in its entirety. The text on the Regulation which the Presidency submits for approval as a General Approach appears in annex,' 201 pages, 11 June 2015, PDF". Archived from the original on 25 December 2015. Retrieved 30 December 2015.
 "The UK GDPR". Information Commissioner's Office ico. 28 June 2021. Retrieved 3 May 2024.
 Francesca Lucarini, "The differences between the California Consumer Privacy Act and the GDPR" Archived 12 July 2020 at the Wayback Machine, Adviser
 Article 3(2): This Regulation applies to the processing of personal data of data subjects who are in the Union by a controller or processor not established in the Union, where the processing activities are related to: (a) the offering of goods or services, irrespective of whether a payment of the data subject is required, to such data subjects in the Union; or (b) the monitoring of their behaviour as far as their behaviour takes place within the Union.
 "What is personal data?". January 2021. Archived from the original on 24 July 2019. Retrieved 22 July 2019.
 "EUR-Lex – 32016R0679 – EN – EUR-Lex". eur-lex.europa.eu. Archived from the original on 17 March 2018. Retrieved 21 March 2018.
 "REGULATION (EU) 2016/679 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL (article 30)". Archived from the original on 28 June 2017. Retrieved 7 June 2017.  Text was copied from this source, which is available under a Creative Commons Attribution 4.0 International License Archived 16 October 2017 at the Wayback Machine.
 "Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA". 4 May 2016.
 The Proposed EU General Data Protection Regulation. A guide for in-house lawyers, Hunton & Williams LLP, June 2015, p. 14
 "Data protection" (PDF). European Commission – European Commission. Archived (PDF) from the original on 3 December 2012. Retrieved 3 January 2013.
 "EUR-Lex – 32016R0679 – EN – EUR-Lex". eur-lex.europa.eu. Archived from the original on 6 November 2017. Retrieved 7 November 2017..
 "newsmyynews". Archived from the original on 22 October 2020. Retrieved 21 October 2020.
 "General_Data_Protection_Regulation". Archived from the original on 22 October 2020. Retrieved 21 October 2020.
 "Age of consent in the GDPR: updated mapping". iapp.org. Archived from the original on 27 May 2018. Retrieved 26 May 2018.
 "How the Proposed EU Data Protection Regulation Is Creating a Ripple Effect Worldwide" Archived 17 February 2021 at the Wayback Machine. Judy Schmitt, Florian Stahl. 11 October 2012. Retrieved 3 January 2013.
 Hern, Alex (21 May 2018). "Most GDPR emails unnecessary and some illegal, say experts". The Guardian. Archived from the original on 28 May 2018. Retrieved 28 May 2018.
 Kamleitner, Bernadette; Mitchell, Vince (1 October 2019). "Your Data Is My Data: A Framework for Addressing Interdependent Privacy Infringements". Journal of Public Policy & Marketing. 38 (4): 433–450. doi:10.1177/0743915619858924. ISSN 0743-9156. S2CID 201343307.
 "Official Journal L 119/2016". eur-lex.europa.eu. Archived from the original on 22 November 2018. Retrieved 26 May 2018.
 "Guidelines on the right to data portability under Regulation 2016/679". European Commission. 2017. Archived from the original on 29 June 2017. Retrieved 2 November 2023.
 Veale, Michael; Binns, Reuben; Ausloos, Jef (2018). "When data protection by design and data subject rights clash". International Data Privacy Law. 8 (2): 105–123. doi:10.1093/idpl/ipy002.
 Zuiderveen Borgesius, Frederik J. (April 2016). "Singling out people without knowing their names – Behavioural targeting, pseudonymous data, and the new Data Protection Regulation". Computer Law & Security Review. 32 (2): 256–271. doi:10.1016/j.clsr.2015.12.013. ISSN 0267-3649.
 Proposal for the EU General Data Protection Regulation Archived 3 December 2012 at the Wayback Machine. European Commission. 25 January 2012. Retrieved 3 January 2013.
 Baldry, Tony; Hyams, Oliver (15 May 2014). "The Right to Be Forgotten". 1 Essex Court. Archived from the original on 19 October 2017. Retrieved 1 June 2014.
 "European Parliament legislative resolution of 12 March 2014 on the proposal for a regulation of the European Parliament and of the Council on the protection of individuals with regard to the processing of personal data and on the free movement of such data (General Data Protection Regulation)". European Parliament. Archived from the original on 5 June 2014. Retrieved 1 June 2014.
 "Practical Data Privacy | Books by Thoughtworkers". Thoughtworks. Retrieved 25 August 2023.
 "Right to object". ico.org.uk. 30 August 2019. Archived from the original on 2 December 2019. Retrieved 14 November 2019.
 Sookman, Barry; Charles Morgan; Adam Goldenberg (30 April 2021). "Using privacy laws to regulate automated decision making". Barry Sookman. Archived from the original on 24 May 2021. Retrieved 24 May 2021.
 Judgment of the Court (Third Chamber) of 4 May 2023. UI v Österreichische Post AG. Request for a preliminary ruling from the Oberster Gerichtshof. Case C-300/21, ECLI:EU:C:2023:370: Reference for a preliminary ruling – Protection of natural persons with regard to the processing of personal data – Regulation (EU) 2016/679 - Article 82(1) - Right to compensation for damage caused by data processing that infringes that regulation - Conditions governing the right to compensation – Mere infringement of that regulation not sufficient - Need for damage caused by that infringement - Compensation for non-material damage resulting from such processing - Incompatibility of a national rule making compensation for such damage subject to the exceeding of a threshold of seriousness - Rules for the determination of damages by national courts.
 Österreichische Post (C-300/21), sub 54.
 Opinion of Advocate General Campos Sánchez-Bordona delivered on 25 May 2023. ZQ v Medizinischer Dienst der Krankenversicherung Nordrhein, Körperschaft des öffentlichen Rechts. Case C‑667/21, ECLI:EU:C:2023:433 (Provisional text)
 "Privacy notices under the EU General Data Protection Regulation". ico.org.uk. 19 January 2018. Archived from the original on 23 May 2018. Retrieved 22 May 2018.
 "What information must be given to individuals whose data is collected?". Europa (web portal). Archived from the original on 23 May 2018. Retrieved 23 May 2018.
 "Privacy and Data Protection by Design – ENISA". Europa (web portal). Archived from the original on 5 April 2017. Retrieved 4 April 2017.
 Data science under GDPR with pseudonymization in the data pipeline Archived 18 April 2018 at the Wayback Machine Published by Dativa, 17 April 2018
 Wes, Matt (25 April 2017). "Looking to comply with GDPR? Here's a primer on anonymization and pseudonymization". IAPP. Archived from the original on 19 February 2018. Retrieved 19 February 2018.
 "Secure personal data | European Data Protection Board". www.edpb.europa.eu. Retrieved 16 May 2024.
 "Data protection by design and default". ico.org.uk. 1 July 2023. Retrieved 16 May 2024.
 "How to protect your privacy online". onerep. 4 September 2023. Retrieved 16 May 2024.
 "What if somebody withdraws their consent? - European Commission". commission.europa.eu. Retrieved 16 May 2024.
 "EUR-Lex – Art. 37". eur-lex.europa.eu. Archived from the original on 22 January 2017. Retrieved 23 January 2017.
 "Explaining GDPR Data Subject Requests". TrueVault. Archived from the original on 20 February 2019. Retrieved 19 February 2019.
 "Guidelines on Data Protection Officers ('DPOs')". Archived from the original on 29 June 2017. Retrieved 2 November 2023.
 Jankowski, Piper-Meredith (21 June 2017). "reach of the GDPR: What is at stake?". Lexology. Archived from the original on 26 May 2018. Retrieved 25 May 2018.
 "L_2016119EN.01000101.xml". eur-lex.europa.eu. Archived from the original on 10 November 2017. Retrieved 28 August 2016.
 "Exemptions". ico.org.uk. 20 July 2020. Archived from the original on 11 November 2020. Retrieved 11 November 2020.
 "The 'Household Exemption' In GDPR". Fenech Farrugia Fiott Legal. 22 May 2020. Archived from the original on 29 October 2020. Retrieved 11 November 2020.
 Wehlander, Caroline (2016). ""Economic Activity": Criteria and Relevance in the Fields of EU Internal Market Law, Competition Law and Procurement Law" (PDF). In Wehlander, Caroline (ed.). Services of General Economic Interest as a Constitutional Concept of EU Law. The Hague, Netherlands: TMC Asser Press. pp. 35–65. doi:10.1007/978-94-6265-117-3_2. ISBN 978-94-6265-116-6. Archived (PDF) from the original on 26 May 2018. Retrieved 23 May 2018.
 "The (Extra) Territorial Scope of the GDPR: The Right to Be Forgotten". Fasken.com. 28 November 2019. Archived from the original on 21 February 2020. Retrieved 21 February 2020.
 "Extraterritorial Scope of GDPR: Do Businesses Outside the EU Need to Comply?". American Bar Association. Archived from the original on 21 February 2020. Retrieved 21 February 2020.
 Art. 27(4) GDPR.
 Art. 27(1) GDPR.
 Art. 83(1), (2) & (4a) GDPR.
 Art. 27(2) GDPR.
 "UK: Understanding the full impact of Brexit on UK: EU data flows". Privacy Matters. DLA Piper. 23 September 2019. Archived from the original on 20 February 2020. Retrieved 20 February 2020.
 Palmer, Danny. "On data protection, the UK says it will go it alone. It probably won't". ZDNet. Archived from the original on 16 February 2020. Retrieved 20 February 2020.
 Donnelly, Conor (18 January 2018). "How to transfer data to a 'third country' under the GDPR". IT Governance Blog En. Archived from the original on 21 February 2020. Retrieved 21 February 2020.
 "Digital Rights post-Brexit". Youtube. Open Rights Group. Archived from the original on 22 November 2022. Retrieved 27 November 2022. Video from Open Rights Group developed as an explainer of the UK's proposals
 "New Data Protection Act finalised in the UK". Out-Law.com. Archived from the original on 25 May 2018. Retrieved 25 May 2018.
 Ashford, Warwick (24 May 2018). "New UK Data Protection Act not welcomed by all". Computer Weekly. Archived from the original on 24 May 2018. Retrieved 25 May 2018.
 Porter, Jon (20 February 2020). "Google shifts authority over UK user data to the US in wake of Brexit". The Verge. Archived from the original on 20 February 2020. Retrieved 20 February 2020.
 "Under-18s face 'like' and 'streaks' limits". BBC News. 15 April 2019. Archived from the original on 15 April 2019. Retrieved 15 April 2019.
 Greenfield, Patrick (15 April 2019). "Facebook urged to disable 'like' feature for child users". The Guardian. ISSN 0261-3077. Archived from the original on 15 April 2019. Retrieved 15 April 2019.
 Afifi-Sabet, Keumars (12 March 2021). "UK seeks divergence from GDPR to 'fuel growth'". IT PRO. Archived from the original on 13 March 2021. Retrieved 12 March 2021.
 "Data sharing myths busted". Information Commissioner's Office. 19 May 2023. Retrieved 19 October 2023.
 "Top nine GDPR myths busted". WS Law. 22 January 2019. Retrieved 19 October 2023.
 "Five GDPR myth-busters". Field Fisher. 11 May 2023. Retrieved 19 October 2023.
 Article 3 (2) of the GDPR
 Gooch, Peter (2018). "A new era for privacy - GDPR six months on" (PDF). Deloitte UK. Archived (PDF) from the original on 12 October 2020. Retrieved 26 November 2020.
 "How Smart Businesses Can Avoid GDPR Penalties When Recording Calls". xewave.io. Archived from the original on 14 April 2018. Retrieved 13 April 2018.
 Babel, Chris (11 July 2017). "The High Costs of GDPR Compliance". InformationWeek. UBM Technology Group. Archived from the original on 5 October 2017. Retrieved 4 October 2017.
 "Preparing for New Privacy Regimes: Privacy Professionals' Views on the General Data Protection Regulation and Privacy Shield" (PDF). bakermckenzie.com. Baker & McKenzie. 4 May 2016. Archived (PDF) from the original on 31 August 2018. Retrieved 4 October 2017.
 Georgiev, Georgi. "GDPR Compliance Cost Calculator". GIGAcalculator.com. Archived from the original on 16 May 2018. Retrieved 16 May 2018.
 Solon, Olivia (19 April 2018). "How Europe's 'breakthrough' privacy law takes on Facebook and Google". The Guardian. Archived from the original on 26 May 2018. Retrieved 25 May 2018.
 "Europe's new privacy rules are no silver bullet". Politico.eu. 22 April 2018. Archived from the original on 26 May 2018. Retrieved 25 May 2018.
 "Lack of GDPR knowledge is a danger and an opportunity". MicroscopeUK. Archived from the original on 26 May 2018. Retrieved 25 May 2018.
 Jeong, Sarah (22 May 2018). "No one's ready for GDPR". The Verge. Archived from the original on 28 May 2018. Retrieved 1 June 2018.
 Edwards, Elaine (22 February 2018). "New rules on data protection pose compliance issues for firms". The Irish Times. Archived from the original on 26 May 2018. Retrieved 25 May 2018.
 Chassang, Gauthier (2017). "The impact of the EU general data protection regulation on scientific research". Ecancermedicalscience. 11: 709. doi:10.3332/ecancer.2017.709. ISSN 1754-6605. PMC 5243137. PMID 28144283.
 Tarhonen, Laura (2017). "Pseudonymisation of Personal Data According to the General Data Protection Regulation". Archived from the original on 19 February 2018. Retrieved 19 February 2018.
 "A recent report issued by the Blockchain Association of Ireland has found there are many more questions than answers when it comes to GDPR". siliconrepublic.com. 23 November 2017. Archived from the original on 5 March 2018. Retrieved 5 March 2018.
 Sample, Ian (27 January 2017). "AI watchdog needed to regulate automated decision-making, say experts". The Guardian. ISSN 0261-3077. Archived from the original on 18 June 2017. Retrieved 15 July 2017.
 "EU's Right to Explanation: A Harmful Restriction on Artificial Intelligence". techzone360.com. Archived from the original on 4 August 2017. Retrieved 15 July 2017.
 Wachter, Sandra; Mittelstadt, Brent; Floridi, Luciano (28 December 2016). "Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation". International Data Privacy Law. SSRN 2903469.
 Edwards, Lilian; Veale, Michael (2017). "Slave to the algorithm? Why a "right to an explanation" is probably not the remedy you are looking for". Duke Law and Technology Review. doi:10.2139/ssrn.2972855. SSRN 2972855.
 Frimin, Michael (29 March 2018). "Five benefits GDPR compliance will bring to your business". Forbes. Archived from the original on 12 September 2018. Retrieved 11 September 2018.
 Butterworth, Trevor (23 May 2018). "Europe's tough new digital privacy law should be a model for US policymakers". Vox. Archived from the original on 12 September 2018. Retrieved 11 September 2018.
 Jaffe, Justin; Hautala, Laura (25 May 2018). "What the GDPR means for Facebook, the EU and you". CNET. Archived from the original on 12 September 2018. Retrieved 11 September 2018.
 "Facebook CEO Zuckerberg's Call for GDPR Privacy Laws Raises Questions". www.cnbc.com. April 2019. Archived from the original on 4 April 2019. Retrieved 8 April 2019.
 Tiku, Nitasha (19 March 2018). "Europe's new privacy law will change the web, and more". Wired. Archived from the original on 15 October 2018. Retrieved 11 September 2018.
 Kalyanpur, Nikhil; Newman, Abraham (25 May 2018). "Today, a new E.U. law transforms privacy rights for everyone. Without Edward Snowden, it might never have happened". The Washington Post. Archived from the original on 11 October 2018. Retrieved 11 September 2018.
 Stallman, Richard (3 April 2018). "A radical proposal to keep your personal data safe". The Guardian. Archived from the original on 12 September 2018. Retrieved 11 September 2018.
 Hoofnagle, Chris Jay; van der Sloot, Bart; Borgesius, Frederik Zuiderveen (10 February 2019). "The European Union general data protection regulation: what it is and what it means". Information & Communications Technology Law. 28 (1): 65–98.
 Afifi-Sabet, Keumars (3 May 2018). "Scammers are using GDPR email alerts to conduct phishing attacks". IT PRO. Archived from the original on 26 May 2018. Retrieved 25 May 2018.
 "EU gov't and public health sites are lousy with adtech, study finds". TechCrunch. 18 March 2019. Archived from the original on 10 April 2021. Retrieved 18 March 2019.
 "EU citizens being tracked on sensitive government websites". Financial Times. 18 March 2019. Archived from the original on 19 March 2019. Retrieved 18 March 2019.
 "Fall asleep in seconds by listening to a soothing voice read the EU's new GDPR legislation". The Verge. Archived from the original on 17 June 2018. Retrieved 16 June 2018.
 "How Europe's GDPR Regulations Became a Meme". Wired. Archived from the original on 18 June 2018. Retrieved 17 June 2018.
 "The Internet Created a GDPR-Inspired Meme Using Privacy Policies". Adweek. Archived from the original on 17 June 2018. Retrieved 17 June 2018.
 Burgess, Matt. "Help, my lightbulbs are dead! How GDPR became bigger than Beyonce". Wired.co.uk. Archived from the original on 19 June 2018. Retrieved 17 June 2018.
 "Here Are Some of the Worst Attempts At Complying with GDPR". Motherboard. 25 May 2018. Archived from the original on 18 June 2018. Retrieved 17 June 2018.
 "What Percentage of Your Software Vulnerabilities Have GDPR Implications?" (PDF). HackerOne. 16 January 2018. Archived (PDF) from the original on 6 July 2018. Retrieved 6 July 2018.
 "The Data Protection Officer (DPO): Everything You Need to Know". Cranium and HackerOne. 20 March 2018. Archived from the original on 31 August 2018. Retrieved 6 July 2018.
 "What might bug bounty programs look like under the GDPR?". The International Association of Privacy Professionals (IAPP). 27 March 2018. Archived from the original on 6 July 2018. Retrieved 6 July 2018.
 Momen, N.; Hatamian, M.; Fritsch, L. (November 2019). "Did App Privacy Improve After the GDPR?". IEEE Security Privacy. 17 (6): 10–20. doi:10.1109/MSEC.2019.2938445. ISSN 1558-4046. S2CID 203699369.
 Hatamian, Majid; Momen, Nurul; Fritsch, Lothar; Rannenberg, Kai (2019), Naldi, Maurizio; Italiano, Giuseppe F.; Rannenberg, Kai; Medina, Manel (eds.), "A Multilateral Privacy Impact Analysis Method for Android Apps", Privacy Technologies and Policy, Lecture Notes in Computer Science, vol. 11498, Springer International Publishing, pp. 87–106, doi:10.1007/978-3-030-21752-5_7, ISBN 978-3-030-21751-8, S2CID 184483219, archived from the original on 12 July 2020, retrieved 3 June 2020
 Moen, Gro Mette, Ailo Krogh Ravna, and Finn Myrstad. "Deceived by design - How tech companies use dark patterns to discourage us from exercising our rights to privacy" Archived 20 December 2019 at the Wayback Machine. 2018. Report by the Norwegian Consumer Council.
 "Instapaper is temporarily shutting off access for European users due to GDPR". The Verge. Archived from the original on 24 May 2018. Retrieved 24 May 2018.
 "Unroll.me to close to EU users saying it can't comply with GDPR". TechCrunch. 5 May 2018. Archived from the original on 30 May 2018. Retrieved 29 May 2018.
 Hern, Alex; Waterson, Jim (24 May 2018). "Sites block users, shut down activities and flood inboxes as GDPR rules loom". The Guardian. Archived from the original on 24 May 2018. Retrieved 25 May 2018.
 "Blocking 500 Million Users Is Easier Than Complying With Europe's New Rules". Bloomberg L.P. 25 May 2018. Archived from the original on 25 May 2018. Retrieved 26 May 2018.
 "U.S. News Outlets Block European Readers Over New Privacy Rules". The New York Times. 25 May 2018. ISSN 0362-4331. Archived from the original on 26 May 2018. Retrieved 26 May 2018.
 "Look: Here's what EU citizens see now that GDPR has landed". Advertising Age. Archived from the original on 25 May 2018. Retrieved 26 May 2018.
 Tiku, Nitasha (24 May 2018). "Why Your Inbox Is Crammed Full of Privacy Policies". Wired. Archived from the original on 24 May 2018. Retrieved 25 May 2018.
 Chen, Brian X. (23 May 2018). "Getting a Flood of G.D.P.R.-Related Privacy Policy Updates? Read Them". The New York Times. ISSN 0362-4331. Archived from the original on 24 May 2018. Retrieved 25 May 2018.
 Lanxon, Nate (25 May 2018). "Blocking 500 Million Users Is Easier Than Complying With Europe's New Rules". Bloomberg. Archived from the original on 25 May 2018. Retrieved 25 May 2018.
 "GDPR mayhem: Programmatic ad buying plummets in Europe". Digiday. 25 May 2018. Archived from the original on 25 May 2018. Retrieved 26 May 2018.
 Skiera, Bernd; Miller, Klaus Matthias; Jin, Yuxi; Kraft, Lennart; Laub, René; Schmitt, Julia (5 July 2022). The impact of the GDPR on the online advertising market. Frankfurt am Main: Bernd Skiera. ISBN 978-3-9824173-0-1. OCLC 1322186902.
 "Press corner". European Commission - European Commission. Archived from the original on 27 December 2020. Retrieved 18 September 2020.
 "Your rights matter: Data protection and privacy - Fundamental Rights Survey". European Union Agency for Fundamental Rights. 12 June 2020. Archived from the original on 25 September 2020. Retrieved 18 September 2020.
 "GDPR: noyb.eu filed four complaints over 'forced consent' against Google, Instagram, WhatsApp and Facebook" (PDF). NOYB.eu. 25 May 2018. Archived from the original (PDF) on 25 May 2018. Retrieved 26 May 2018.
 "Facebook and Google hit with $8.8 billion in lawsuits on day one of GDPR". The Verge. Archived from the original on 25 May 2018. Retrieved 26 May 2018.
 "Max Schrems files first cases under GDPR against Facebook and Google". The Irish Times. Archived from the original on 25 May 2018. Retrieved 26 May 2018.
 "Facebook, Google face first GDPR complaints over 'forced consent'". TechCrunch. 25 May 2018. Archived from the original on 26 May 2018. Retrieved 26 May 2018.
 Meyer, David. "Google, Facebook hit with serious GDPR complaints: Others will be soon". ZDNet. Archived from the original on 28 May 2018. Retrieved 26 May 2018.
 Fox, Chris (21 January 2019). "Google hit with £44m GDPR fine". BBC News. Archived from the original on 21 January 2019. Retrieved 14 June 2019.
 Porter, Jon (21 January 2019). "Google fined €50 million for GDPR violation in France". The Verge. Archived from the original on 10 June 2019. Retrieved 14 June 2019.
 Masnick, Mike (19 November 2018). "Yet Another GDPR Disaster: Journalists Ordered To Hand Over Secret Sources Under 'Data Protection' Law". Archived from the original on 20 November 2018. Retrieved 20 November 2018.
 Bălăiți, George (9 November 2018). "English Translation of the Letter from the Romanian Data Protection Authority to RISE Project". Organized Crime and Corruption Reporting Project. Archived from the original on 9 November 2018. Retrieved 20 November 2018.
 "Intention to fine British Airways £183.39m under GDPR for data breach". ICO. 8 July 2019. Archived from the original on 6 December 2020. Retrieved 22 December 2020.
 Whittaker, Zack (11 September 2018). "British Airways breach caused by credit card skimming malware, researchers say". TechCrunch. Archived from the original on 10 December 2018. Retrieved 9 December 2018.
 "British Airways boss apologises for 'malicious' data breach". BBC News. 7 September 2018. Archived from the original on 15 October 2018. Retrieved 7 September 2018.
 Sweney, Mark (8 July 2019). "BA faces £183m fine over passenger data breach". The Guardian. ISSN 0261-3077. Archived from the original on 8 July 2019. Retrieved 8 July 2019.
 "British Airways faces record £183m fine for data breach". BBC News. 8 July 2019. Archived from the original on 8 July 2019. Retrieved 8 July 2019.
 "ICO fines British Airways £20m for data breach affecting more than 400,000 customers". ICO. 16 October 2020. Archived from the original on 16 October 2020. Retrieved 22 December 2020.
 Vinocur, Nicholas (27 December 2019). "'We have a huge problem': European regulator despairs over lack of enforcement". Politico. Archived from the original on 28 December 2019. Retrieved 6 May 2020.
 Ryan, Johnny (31 January 2023). "Europe-wide overhaul of GDPR monitoring triggered by ICCL". Irish Council for Civil Liberties. Archived from the original on 6 April 2023. Retrieved 8 April 2023.
 Alizadeh, Fatemeh; Jakobi, Timo; Boldt, Jens; Stevens, Gunnar (2019). "GDPR-Reality Check on the Right to Access Data". Proceedings of Mensch und Computer 2019. New York: ACM Press. pp. 811–814. doi:10.1145/3340764.3344913. ISBN 978-1-4503-7198-8. S2CID 202159324.
 Alizadeh, Fatemeh; Jakobi, Timo; Boden, Alexander; Stevens, Gunnar; Boldt, Jens (2020). "GDPR Reality Check–Claiming and Investigating Personally Identifiable Data from Companies" (PDF). EuroUSEC. Archived (PDF) from the original on 17 June 2020. Retrieved 17 June 2020.
 Human, Soheil; Cech, Florian (2021). "A Human-Centric Perspective on Digital Consenting: The Case of GAFAM" (PDF). In Zimmermann, Alfred; Howlett, Robert J.; Jain, Lakhmi C. (eds.). Human Centred Intelligent Systems. Smart Innovation, Systems and Technologies. Vol. 189. Singapore: Springer. pp. 139–159. doi:10.1007/978-981-15-5784-2_12. ISBN 978-981-15-5784-2. S2CID 214699040. Archived (PDF) from the original on 14 April 2021. Retrieved 23 August 2020.
 Christakis and Propp, Theodore and Kenneth (8 March 2021). "How Europe's Intelligence Services Aim to Avoid the EU's Highest Court—and What It Means for the United States". Lawfare. Archived from the original on 23 September 2023. Retrieved 13 March 2021.
 Browne, Ryan (18 January 2022). "Fines for breaches of EU privacy law spike sevenfold to $1.2 billion, as Big Tech bears the brunt". CNBC. Archived from the original on 9 February 2022. Retrieved 9 February 2022.
 Roberts, Jeff John (25 May 2018). "The GDPR Is in Effect: Should U.S. Companies Be Afraid?". Archived from the original on 28 May 2018. Retrieved 28 May 2018.
 "Commentary: California's New Data Privacy Law Could Begin a Regulatory Disaster". Fortune. Archived from the original on 10 April 2019. Retrieved 10 April 2019.
 "California Unanimously Passes Historic Privacy Bill". Wired. Archived from the original on 29 June 2018. Retrieved 29 June 2018.
 "Marketers and tech companies confront California's version of GDPR". Archived from the original on 29 June 2018. Retrieved 29 June 2018.
 "Virginia passes the Consumer Data Protection Act". International Association of Privacy Professionals. 3 March 2021. Archived from the original on 30 August 2021. Retrieved 26 August 2021.
 "Colorado Privacy Act becomes law". International Association of Privacy Professionals. 8 July 2021. Archived from the original on 26 August 2021. Retrieved 26 August 2021.
 "KİŞİSEL VERİLERİ KORUMA KURUMU | KVKK | History". www.kvkk.gov.tr. Retrieved 19 December 2020.
 Zhang, Angela Huyue (2024). High Wire: How China Regulates Big Tech and Governs Its Economy. Oxford University Press. ISBN 9780197682258.
 Portal, S. M. E. "New Federal Act on Data Protection (nFADP)". www.kmu.admin.ch. Archived from the original on 25 March 2023. Retrieved 25 March 2023.
 Goldberg, Samuel G.; Johnson, Garrett A.; Shriver, Scott K. (2024). "Regulating Privacy Online: An Economic Evaluation of the GDPR". American Economic Journal: Economic Policy. 16 (1): 325–358. doi:10.1257/pol.20210309. ISSN 1945-7731.
 "Data protection reform: Council adopts position at first reading – Consilium". Europa (web portal). Archived from the original on 6 October 2017. Retrieved 14 April 2016.
 Adoption of the Council's position at first reading Archived 25 November 2017 at the Wayback Machine, Votewatch.eu
 Written procedure Archived 1 December 2017 at the Wayback Machine, 8 April 2016, Council of the European Union
 "Data protection reform – Parliament approves new rules fit for the digital era – News – European Parliament". 14 April 2016. Archived from the original on 17 April 2016. Retrieved 14 April 2016.
 "The History of the General Data Protection Regulation | European Data Protection Supervisor". edps.europa.eu. 25 May 2018. Retrieved 2 February 2024.
 "General Data Protection Regulation (GDPR) entered into force in the EEA". EFTA. 20 July 2018. Archived from the original on 1 October 2018. Retrieved 30 September 2018.
 Kolsrud, Kjetil (10 July 2018). "GDPR – 20. juli er datoen!". Rett24. Archived from the original on 13 July 2018. Retrieved 13 July 2018.
 "Digital Single Market". Digital Single Market. Archived from the original on 8 October 2017. Retrieved 5 October 2017.
 "What does the ePrivacy Regulation mean for the online industry? – ePrivacy". www.eprivacy.eu. Archived from the original on 22 May 2018. Retrieved 26 May 2018.
 "Council position and findings on the application of the General Data Protection Regulation (GDPR), 19 December 2019". Consilium. Archived from the original on 23 December 2019. Retrieved 23 December 2019.
External links

Wikimedia Commons has media related to General Data Protection Regulation.
General Data Protection Regulation consolidated text on EUR-Lex
General Data Protection Regulation initial legal act in the OJEU
Data protection, European Commission
Procedure 2012/0011/COD, EUR-Lex
Handbook on European data protection law, European Union Agency for Fundamental Rights
vte
Privacy
Authority control databases Edit this at Wikidata
Categories: Privacy lawInformation privacyEuropean Union regulationsData laws of EuropeData protection2016 establishments in Europe2018 in EuropeJuncker CommissionRegulation of artificial intelligence


=========================================================================================================
Health Insurance Portability and Accountability Act

Article
Talk
Read
Edit
View history

Tools
From Wikipedia, the free encyclopedia
Health Insurance Portability and Accountability Act of 1996
Great Seal of the United States
Other short titles	Kassebaum–Kennedy Act, Kennedy–Kassebaum Act
Long title	An Act To amend the Internal Revenue Code of 1986 to improve portability and continuity of health insurance coverage in the group and individual markets, to combat waste, fraud, and abuse in health insurance and health care delivery, to promote the use of medical savings accounts, to improve access to long-term care services and coverage, to simplify the administration of health insurance, and for other purposes.
Acronyms (colloquial)	HIPAA (pronounced /ˈhɪpə/ HIP-uh)
Enacted by	the 104th United States Congress
Citations
Public law	Pub. L.Tooltip Public Law (United States) 104–191 (text) (PDF)
Statutes at Large	110 Stat. 1936
Legislative history
Introduced in the House as H.R. 3103 by Bill Archer (R-TX) on March 18, 1996
Committee consideration by House Ways and Means
Passed the House on March 28, 1996 (267–151)
Passed the Senate on April 23, 1996 (100–0, in lieu of S. 1028)
Reported by the joint conference committee on July 31, 1996; agreed to by the House on August 1, 1996 (421–2) and by the Senate on August 2, 1996 (98–0)
Signed into law by President Bill Clinton on August 21, 1996
The Health Insurance Portability and Accountability Act of 1996 (HIPAA or the Kennedy–Kassebaum Act[1][2]) is a United States Act of Congress enacted by the 104th United States Congress and signed into law by President Bill Clinton on August 21, 1996.[3] It aimed to alter the transfer of healthcare information, stipulated the guidelines by which personally identifiable information maintained by the healthcare and healthcare insurance industries should be protected from fraud and theft,[4] and addressed some limitations on healthcare insurance coverage. It generally prohibits healthcare providers and businesses called covered entities from disclosing protected information to anyone other than a patient and the patient's authorized representatives without their consent. The bill does not restrict patients from receiving information about themselves (with limited exceptions).[5] Furthermore, it does not prohibit patients from voluntarily sharing their health information however they choose, nor does it require confidentiality where a patient discloses medical information to family members, friends or other individuals not employees of a covered entity.

The act consists of 5 titles:

Title I protects health insurance coverage for workers and their families when they change or lose their jobs.[6]
Title II, known as the Administrative Simplification (AS) provisions, requires the establishment of national standards for electronic health care transactions and national identifiers for providers, health insurance plans, and employers.[7]
Title III sets guidelines for pre-tax medical spending accounts.
Title IV sets guidelines for group health plans.
Title V governs company-owned life insurance policies.
Titles
There are five sections to the act, known as titles.

Title I: Health Care Access, Portability, and Renewability
Title I of HIPAA regulates the availability and breadth of group health plans and certain individual health insurance policies. It amended the Employee Retirement Income Security Act, the Public Health Service Act, and the Internal Revenue Code. Furthermore, Title I addresses the issue of "job lock" which is the inability for an employee to leave their job because they would lose their health coverage.[8] To combat the job lock issue, the Title protects health insurance coverage for workers and their families if they lose or change their jobs.[9]

Title I requires the coverage of and also limits restrictions that a group health plan can place on benefits for preexisting conditions. Group health plans may refuse to provide benefits in relation to preexisting conditions for either 12 months following enrollment in the plan or 18 months in the case of late enrollment.[10] Title I allows individuals to reduce the exclusion period by the amount of time that they have had "creditable coverage" before enrolling in the plan and after any "significant breaks" in coverage.[11] "Creditable coverage" is defined quite broadly and includes nearly all group and individual health plans, Medicare, and Medicaid.[12] A "significant break" in coverage is defined as any 63-day period without any creditable coverage.[13] Along with an exception, it allows employers to tie premiums or co-payments to tobacco use, or body mass index.

Title I mandates that insurance providers must issue policies without exclusions to individuals leaving group health plans, provided they have maintained continuous, credible coverage. (see above) exceeding 18 months, and[14] renew individual policies for as long as they are offered or provide alternatives to discontinued plans for as long as the insurer stays in the market without exclusion regardless of health condition.

Some health care plans are exempted from Title I requirements, such as long-term health plans and limited-scope plans like dental or vision plans offered separately from the general health plan. However, if such benefits are part of the general health plan, then HIPAA still applies to such benefits. For example, if the new plan offers dental benefits, then it must count creditable continuous coverage under the old health plan towards any of its exclusion periods for dental benefits.

An alternate method of calculating creditable continuous coverage is available to the health plan under Title I. That is, 5 categories of health coverage can be considered separately, including dental and vision coverage. Anything not under those 5 categories must use the general calculation (e.g., the beneficiary may be counted with 18 months of general coverage, but only 6 months of dental coverage, because the beneficiary did not have a general health plan that covered dental until 6 months prior to the application date). Since limited-coverage plans are exempt from HIPAA requirements, the odd case exists in which the applicant to a general group health plan cannot obtain certificates of creditable continuous coverage for independent limited-scope plans, such as dental to apply towards exclusion periods of the new plan that does include those coverages.

Hidden exclusion periods are not valid under Title I (e.g., "The accident, to be covered, must have occurred while the beneficiary was covered under this exact same health insurance contract"). Such clauses must not be acted upon by the health plan. Also, they must be re-written so they can comply with HIPAA.[15]

Title II: Preventing Health Care Fraud and Abuse; Administrative Simplification; Medical Liability Reform

This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources in this section. Unsourced material may be challenged and removed. (April 2010) (Learn how and when to remove this message)
Title II of HIPAA establishes policies and procedures for maintaining the privacy and the security of individually identifiable health information, outlines numerous offenses relating to health care, and establishes civil and criminal penalties for violations. It also creates several programs to control fraud and abuse within the health-care system.[16][17][18][19] However, the most significant provisions of Title II are its Administrative Simplification rules. Title II requires the Department of Health and Human Services (HHS) to increase the efficiency of the health-care system by creating standards for the use and dissemination of health-care information.[19]

These rules apply to "covered entities", as defined by HIPAA and the HHS. Covered entities include health plans, health care clearinghouses (such as billing services and community health information systems), and health care providers that transmit health care data in a way regulated by HIPAA.[20]

Per the requirements of Title II, the HHS has promulgated five rules regarding Administrative Simplification: the Privacy Rule, the Transactions and Code Sets Rule, the Security Rule, the Unique Identifiers Rule, and the Enforcement Rule.[21]

Privacy Rule
The HIPAA Privacy Rule is composed of national regulations for the use and disclosure of Protected Health Information (PHI) in healthcare treatment, payment and operations by "covered entities" (generally, health care clearinghouses, employer-sponsored health plans, health insurers, and medical service providers that engage in certain transactions).[22]

The Privacy Rule came into effect on April 14, 2003, with a one-year extension for certain "small plans". By regulation, the HHS extended the HIPAA privacy rule to independent contractors of covered entities who fit within the definition of "business associates".[23] PHI is any information that is held by a covered entity regarding health status, provision of health care, or health care payment that can be linked to any individual.[20] This is interpreted rather broadly and includes any part of an individual's medical record or payment history. Covered entities must disclose PHI to the individual within 30 days upon request.[24] They must also disclose PHI when required to do so by law such as reporting suspected child abuse to state child welfare agencies.[25]

Covered entities may disclose protected health information to law enforcement officials for law enforcement purposes as required by law (including court orders, court-ordered warrants, subpoenas) and administrative requests; or to identify or locate a suspect, a fugitive, a material witness, or a missing person.[26]

A covered entity may disclose PHI to certain parties to facilitate treatment, payment, or health care operations without a patient's express written authorization.[27] Any other disclosures of PHI require the covered entity to obtain written authorization from the individual for disclosure.[28] In any case, when a covered entity discloses any PHI, it must make a reasonable effort to disclose only the minimum necessary information required to achieve its purpose.[29]

The Privacy Rule gives individuals the right to request a covered entity to correct any inaccurate PHI.[30] Also, it requires covered entities to take some reasonable steps on ensuring the confidentiality of communications with individuals.[31] For example, an individual can ask to be called at their work number instead of home or cell phone numbers.

The Privacy Rule requires covered entities to notify individuals of uses of their PHI.[32] Covered entities must also keep track of disclosures of PHI and document privacy policies and procedures.[33] They must appoint a Privacy Official and a contact person[34] responsible for receiving complaints and train all members of their workforce in procedures regarding PHI.[35]

An individual who believes that the Privacy Rule is not being upheld can file a complaint with the Department of Health and Human Services Office for Civil Rights (OCR).[36][37] In 2006 the Wall Street Journal reported that the OCR had a long backlog and ignores most complaints. "Complaints of privacy violations have been piling up at the Department of Health and Human Services. Between April of 2003 and November 2006, the agency fielded 23,886 complaints related to medical-privacy rules, but it has not yet taken any enforcement actions against hospitals, doctors, insurers or anyone else for rule violations. A spokesman for the agency says it has closed three-quarters of the complaints, typically because it found no violation or after it provided informal guidance to the parties involved."[38] However, in July 2011, the University of California, Los Angeles agreed to pay $865,500 in a settlement regarding potential HIPAA violations. An HHS Office for Civil Rights investigation showed that from 2005 to 2008, unauthorized employees repeatedly and without legitimate cause looked at the electronic protected health information of numerous UCLAHS patients.[39]

It is a misconception that the Privacy Rule creates a right for any individual to refuse to disclose any health information (such as chronic conditions or immunization records) if requested by an employer or business. HIPAA Privacy Rule requirements merely place restrictions on disclosure by covered entities and their business associates without the consent of the individual whose records are being requested; they do not place any restrictions upon requesting health information directly from the subject of that information.[40][41][42]

2013 Final Omnibus Rule update
In January 2013, HIPAA was updated via the Final Omnibus Rule.[43] The updates included changes to the Security Rule and Breach Notification portions of the HITECH Act. The most significant changes related to the expansion of requirements to include business associates, where only covered entities had originally been held to uphold these sections of the law.[citation needed]

In addition, the definition of "significant harm" to an individual in the analysis of a breach was updated to provide more scrutiny to covered entities with the intent of disclosing breaches that previously were unreported. Previously, an organization needed proof that harm had occurred whereas now organizations must prove that harm had not occurred.

Protection of PHI was changed from indefinite to 50 years after death. More severe penalties for violation of PHI privacy requirements were also approved.[44]

The HIPAA Privacy rule may be waived during disasters. Limited waivers have been issued in cases such as Hurricane Harvey in 2017.[45]

HITECH Act: privacy requirements
See the Privacy section of the Health Information Technology for Economic and Clinical Health Act (HITECH Act).

Right to access one's PHI
The Privacy Rule requires medical providers to give individuals access to their PHI.[46] After an individual requests information in writing (typically using the provider's form for this purpose), a provider has up to 30 days to provide a copy of the information to the individual. An individual may request the information in electronic form or hard-copy, and the provider is obligated to attempt to conform to the requested format. For providers using an electronic health record (EHR) system that is certified using CEHRT (Certified Electronic Health Record Technology) criteria, individuals must be allowed to obtain the PHI in electronic form. Providers are encouraged to provide the information expediently, especially in the case of electronic record requests.

Individuals have the broad right to access their health-related information, including medical records, notes, images, lab results, and insurance and billing information.[47] Explicitly excluded are the private psychotherapy notes of a provider, and information gathered by a provider to defend against a lawsuit.[48]

Providers can charge a reasonable amount that relates to their cost of providing the copy, however, no charge is allowable when providing data electronically from a certified EHR using the "view, download, and transfer" feature which is required for certification. When delivered to the individual in electronic form, the individual may authorize delivery using either encrypted or unencrypted email, delivery using media (USB drive, CD, etc., which may involve a charge), direct messaging (a secure email technology in common use in the healthcare industry), or possibly other methods. When using unencrypted email, the individual must understand and accept the risks to privacy using this technology (the information may be intercepted and examined by others). Regardless of delivery technology, a provider must continue to fully secure the PHI while in their system and can deny the delivery method if it poses additional risk to PHI while in their system.[49]

An individual may also request (in writing) that their PHI is delivered to a designated third party such as a family care provider.

An individual may also request (in writing) that the provider send PHI to a designated service used to collect or manage their records, such as a Personal Health Record application. For example, a patient can request in writing that her ob-gyn provider digitally transmit records of her latest prenatal visit to a pregnancy self-care app that she has on her mobile phone.

Disclosure to relatives
According to their interpretations of HIPAA, hospitals will not reveal information over the phone to relatives of admitted patients. This has, in some instances, impeded the location of missing persons. After the Asiana Airlines Flight 214 San Francisco crash, some hospitals were reluctant to disclose the identities of passengers that they were treating, making it difficult for Asiana and the relatives to locate them.[50] In one instance, a man in Washington state was unable to obtain information about his injured mother.[51]

Janlori Goldman, director of the advocacy group Health Privacy Project, said that some hospitals are being "overcautious" and misapplying the law, the Times reports. Suburban Hospital in Bethesda, Md., has interpreted a federal regulation that requires hospitals to allow patients to opt out of being included in the hospital directory as meaning that patients want to be kept out of the directory unless they specifically say otherwise. As a result, if a patient is unconscious or otherwise unable to choose to be included in the directory, relatives and friends might not be able to find them, Goldman said.[52]

Transactions and Code Sets Rule
HIPAA was intended to make the health care system in the United States more efficient by standardizing health care transactions. HIPAA added a new Part C titled "Administrative Simplification" to Title XI of the Social Security Act.[53] This is supposed to simplify healthcare transactions by requiring all health plans to engage in health care transactions in a standardized way.

The HIPAA/EDI (electronic data interchange) provision was scheduled to take effect from October 16, 2003, with a one-year extension for certain "small plans". However, due to widespread confusion and difficulty in implementing the rule, Centers for Medicare & Medicaid Services (CMS) granted a one-year extension to all parties.[54] On January 1, 2012, newer versions, ASC X12 005010 and NCPDP D.0 become effective, replacing the previous ASC X12 004010 and NCPDP 5.1 mandate.[55] The ASC X12 005010 version provides a mechanism allowing the use of ICD-10-CM as well as other improvements.

Under HIPAA, HIPAA-covered health plans are now required to use standardized HIPAA electronic transactions. See, 42 USC § 1320d-2 and 45 CFR Part 162. Information about this can be found in the final rule for HIPAA electronic transaction standards (74 Fed. Reg. 3296, published in the Federal Register on January 16, 2009), and on the CMS website.[56]

The EDI Health Care Claim Transaction Set (837) is used to submit health care claim billing information, encounter information, or both, except for retail pharmacy claims (see EDI Retail Pharmacy Claim Transaction). It can be sent from providers of health care services to payers, either directly or via intermediary billers and claims clearinghouses. It can also be used to transmit health care claims and billing payment information between payers with different payment responsibilities where coordination of benefits is required or between payers and regulatory agencies to monitor the rendering, billing, and/or payment of health care services within a specific health care/insurance industry segment.

For example, a state mental health agency may mandate all healthcare claims, Providers and health plans who trade professional (medical) health care claims electronically must use the 837 Health Care Claim: Professional standard to send in claims. As there are many different business applications for the Health Care claim, there can be slight derivations to cover off claims involving unique claims such as for institutions, professionals, chiropractors, dentists, etc.

EDI Retail Pharmacy Claim Transaction (NCPDP Telecommunications is used to submit retail pharmacy claims to payers by health care professionals who dispense medications, either directly or via intermediary billers and claims clearinghouses. It can also be used to transmit claims for retail pharmacy services and billing payment information between payers with different payment responsibilities where coordination of benefits is required or between payers and regulatory agencies to monitor the rendering, billing, and/or payment of retail pharmacy services within the pharmacy health care/insurance industry segment.

The EDI Health Care Claim Payment/Advice Transaction Set (835) can be used to make a payment, send an Explanation of Benefits (EOB), send an Explanation of Payments (EOP) remittance advice, or make a payment and send an EOP remittance advice only from a health insurer to a health care provider either directly or via a financial institution.

The EDI Benefit Enrollment and Maintenance Set (834) can be used by employers, unions, government agencies, associations or insurance agencies to enroll members to a payer. The payer is a healthcare organization that pays claims, administers insurance or benefit or product. Examples of payers include an insurance company, healthcare professional (HMO), preferred provider organization (PPO), government agency (Medicaid, Medicare etc.) or any organization that may be contracted by one of these former groups.

EDI Payroll Deducted, and another group, Premium Payment for Insurance Products (820), is a transaction set for making a premium payment for insurance products. It can be used to order a financial institution to make a payment to a payee.

EDI Health Care Eligibility/Benefit Inquiry (270) is used to inquire about the health care benefits and eligibility associated with a subscriber or dependent.

EDI Health Care Eligibility/Benefit Response (271) is used to respond to a request inquiry about the health care benefits and eligibility associated with a subscriber or dependent.

EDI Health Care Claim Status Request (276) is a transaction set that can be used by a provider, recipient of health care products or services or their authorized agent to request the status of a health care claim.

EDI Health Care Claim Status Notification (277) is a transaction set that can be used by a healthcare payer or authorized agent to notify a provider, recipient or authorized agent regarding the status of a health care claim or encounter, or to request additional information from the provider regarding a health care claim or encounter. This transaction set is not intended to replace the Health Care Claim Payment/Advice Transaction Set (835) and therefore, is not used for account payment posting. The notification is at a summary or service line detail level. The notification may be solicited or unsolicited.

EDI Health Care Service Review Information (278) is a transaction set that can be used to transmit health care service information, such as subscriber, patient, demographic, diagnosis or treatment data for the purpose of the request for review, certification, notification or reporting the outcome of a health care services review.

EDI Functional Acknowledgement Transaction Set (997) is a transaction set that can be used to define the control structures for a set of acknowledgments to indicate the results of the syntactical analysis of the electronically encoded documents. Although it's not specifically named in the HIPAA Legislation or Final Rule, it's necessary for X12 transaction set processing. The encoded documents are the transaction sets, which are grouped in functional groups, used in defining transactions for business data interchange. This standard doesn't cover the semantic meaning of the information encoded in the transaction sets.

Brief 5010 Transactions and Code Sets Rules Update Summary
Transaction Set (997) will be replaced by Transaction Set (999) "acknowledgment report".
The size of many fields {segment elements} will be expanded, causing a need for all IT providers to expand corresponding fields, element, files, GUI, paper media, and databases.
Some segments have been removed from existing Transaction Sets.
Many segments have been added to existing Transaction Sets allowing greater tracking and reporting of cost and patient encounters.
Capacity to use both "International Classification of Diseases" versions 9 (ICD-9) and 10 (ICD-10-CM) has been added.[57][58]
Security Rule
The Final Rule on Security Standards was issued on February 20, 2003. The Security Rule complements the Privacy Rule. While the Privacy Rule pertains to all Protected Health Information (PHI) including paper and electronic, the Security Rule deals specifically with Electronic Protected Health Information (EPHI). It lays out three types of security safeguards required for compliance: administrative, physical, and technical.[59] For each of these types, the Rule identifies various security standards, and for each standard, it names both required and addressable implementation specifications. Required specifications must be adopted and administered as dictated by the Rule. Addressable specifications are more flexible. Individual covered entities can evaluate their own situation and determine the best way to implement addressable specifications. Some privacy advocates have argued that this "flexibility" may provide too much latitude to covered entities.[60] Software tools have been developed to assist covered entities in the risk analysis and remediation tracking. The standards and specifications are as follows:

Administrative Safeguards – policies and procedures designed to clearly show how the entity will comply with the act
Covered entities (entities that must comply with HIPAA requirements) must adopt a written set of privacy procedures and designate a privacy officer to be responsible for developing and implementing all required policies and procedures.
The policies and procedures must reference management oversight and organizational buy-in to compliance with the documented security controls.
Procedures should clearly identify employees or classes of employees who have access to electronic protected health information (EPHI). Access to EPHI must be restricted to only those employees who have a need for it to complete their job function.
The procedures must address access authorization, establishment, modification, and termination.
Entities must show that an appropriate ongoing training program regarding the handling of PHI is provided to employees performing health plan administrative functions.
Covered entities that out-source some of their business processes to a third party must ensure that their vendors also have a framework in place to comply with HIPAA requirements. Companies typically gain this assurance through clauses in the contracts stating that the vendor will meet the same data protection requirements that apply to the covered entity. Care must be taken to determine if the vendor further out-sources any data handling functions to other vendors and monitor whether appropriate contracts and controls are in place.
A contingency plan should be in place for responding to emergencies. Covered entities are responsible for backing up their data and having disaster recovery procedures in place. The plan should document data priority and failure analysis, testing activities, and change control procedures.
Internal audits play a key role in HIPAA compliance by reviewing operations with the goal of identifying potential security violations. Policies and procedures should specifically document the scope, frequency, and procedures of audits. Audits should be both routine and event-based.
Procedures should document instructions for addressing and responding to security breaches that are identified either during the audit or the normal course of operations.
Physical Safeguards – controlling physical access to protect against inappropriate access to protected data
Controls must govern the introduction and removal of hardware and software from the network. (When equipment is retired it must be disposed of properly to ensure that PHI is not compromised.)
Access to equipment containing health information should be carefully controlled and monitored.
Access to hardware and software must be limited to properly authorized individuals.
Required access controls consist of facility security plans, maintenance records, and visitor sign-in and escorts.
Policies are required to address proper workstation use. Workstations should be removed from high traffic areas and monitor screens should not be in direct view of the public.
If the covered entities utilize contractors or agents, they too must be fully trained on their physical access responsibilities.
Technical Safeguards – controlling access to computer systems and enabling covered entities to protect communications containing PHI transmitted electronically over open networks from being intercepted by anyone other than the intended recipient.
Information systems housing PHI must be protected from intrusion. When information flows over open networks, some form of encryption must be utilized. If closed systems/networks are utilized, existing access controls are considered sufficient and encryption is optional.
Each covered entity is responsible for ensuring that the data within its systems has not been changed or erased in an unauthorized manner.
Data corroboration, including the use of a checksum, double-keying, message authentication, and digital signature may be used to ensure data integrity.
Covered entities must also authenticate entities with which they communicate. Authentication consists of corroborating that an entity is who it claims to be. Examples of corroboration include password systems, two or three-way handshakes, telephone callback, and token systems.
Covered entities must make documentation of their HIPAA practices available to the government to determine compliance.
In addition to policies and procedures and access records, information technology documentation should also include a written record of all configuration settings on the components of the network because these components are complex, configurable, and always changing.
Documented risk analysis and risk management programs are required. Covered entities must carefully consider the risks of their operations as they implement systems to comply with the act. (The requirement of risk analysis and risk management implies that the act's security requirements are a minimum standard and places responsibility on covered entities to take all reasonable precautions necessary to prevent PHI from being used for non-health purposes.)
Unique Identifiers Rule (National Provider Identifier)
HIPAA covered entities such as providers completing electronic transactions, healthcare clearinghouses, and large health plans must use only the National Provider Identifier (NPI) to identify covered healthcare providers in standard transactions by May 23, 2007. Small health plans must use only the NPI by May 23, 2008. Effective from May 2006 (May 2007 for small health plans), all covered entities using electronic communications (e.g., physicians, hospitals, health insurance companies, and so forth) must use a single new NPI. The NPI replaces all other identifiers used by health plans, Medicare, Medicaid, and other government programs.[61] However, the NPI does not replace a provider's DEA number, state license number, or tax identification number. The NPI is 10 digits (may be alphanumeric), with the last digit being a checksum. The NPI cannot contain any embedded intelligence; in other words, the NPI is simply a number that does not itself have any additional meaning. The NPI is unique and national, never re-used, and except for institutions, a provider usually can have only one. An institution may obtain multiple NPIs for different "sub-parts" such as a free-standing cancer center or rehab facility.

Enforcement Rule
On February 16, 2006, HHS issued the Final Rule regarding HIPAA enforcement. It became effective on March 16, 2006. The Enforcement Rule sets civil money penalties for violating HIPAA rules and establishes procedures for investigations and hearings for HIPAA violations. For many years there were few prosecutions for violations.[62]

This may have changed with the fining of $50,000 to the Hospice of North Idaho (HONI) as the first entity to be fined for a potential HIPAA Security Rule breach affecting fewer than 500 people. Rachel Seeger, a spokeswoman for HHS, stated, "HONI did not conduct an accurate and thorough risk analysis to the confidentiality of ePHI [electronic Protected Health Information] as part of its security management process from 2005 through Jan. 17, 2012." This investigation was initiated with the theft from an employees vehicle of an unencrypted laptop containing 441 patient records.[63]


Wikisource has original text related to this article:
American Recovery and Reinvestment Act of 2009/Division A/Title XIII/Subtitle D
As of March 2013, the United States Department of Health and Human Services (HHS) has investigated over 19,306 cases that have been resolved by requiring changes in privacy practice or by corrective action. If noncompliance is determined by HHS, entities must apply corrective measures. Complaints have been investigated against many different types of businesses such as national pharmacy chains, major health care centers, insurance groups, hospital chains and other small providers. There were 9,146 cases where the HHS investigation found that HIPAA was followed correctly. There were 44,118 cases that HHS did not find eligible cause for enforcement; for example, a violation that started before HIPAA started; cases withdrawn by the pursuer; or an activity that does not actually violate the Rules.

Title III: Tax-related health provisions governing medical savings accounts
Title III standardizes the amount that may be saved per person in a pre-tax medical savings account. Beginning in 1997, a medical savings account ("MSA") became available to employees covered under an employer-sponsored high deductible plan, these being small employer and self-employed individuals.

Title IV: Application and enforcement of group health insurance requirements
Title IV specifies conditions for group health plans regarding coverage of persons with preexisting conditions, and modifies continuation of coverage requirements. It also clarifies continuation coverage requirements and includes COBRA clarification.

Title V: Revenue offset governing tax deductions for employers
Title V includes provisions related to company-owned life insurance for employers providing company-owned life insurance premiums, prohibiting the tax-deduction of interest on life insurance loans, company endowments, or contracts related to the company. It also repeals the financial institution rule to interest allocation rules. Finally, it amends provisions of law relating to people who give up United States citizenship or permanent residence, expanding the expatriation tax to be assessed against those deemed to be giving up their U.S. status for tax reasons, and making ex-citizens' names part of the public record through the creation of the Quarterly Publication of Individuals Who Have Chosen to Expatriate.[64]

Effects on research and clinical care
The enactment of the Privacy and Security Rules has caused major changes in the way physicians and medical centers operate. The complex legalities and potentially stiff penalties associated with HIPAA, as well as the increase in paperwork and the cost of its implementation, were causes for concern among physicians and medical centers. An August 2006 article in the journal Annals of Internal Medicine detailed some such concerns over the implementation and effects of HIPAA.[65]

Effects on research
HIPAA restrictions on researchers have affected their ability to perform retrospective, chart-based research as well as their ability to prospectively evaluate patients by contacting them for follow-up. A study from the University of Michigan demonstrated that implementation of the HIPAA Privacy rule resulted in a drop from 96% to 34% in the proportion of follow-up surveys completed by study patients being followed after a heart attack.[66] Another study, detailing the effects of HIPAA on recruitment for a study on cancer prevention, demonstrated that HIPAA-mandated changes led to a 73% decrease in patient accrual, a tripling of time spent recruiting patients, and a tripling of mean recruitment costs.[67]

Under HIPAA, informed consent forms for research studies must document how protected health information will be kept private, potentially increasing barriers to participation.[65]

These data suggest that HIPAA privacy rules may have negative effects on the cost and quality of medical research. Dr. Kim Eagle, professor of internal medicine at the University of Michigan, was quoted in the Annals article as saying, "Privacy is important, but research is also important for improving care. We hope that we will figure this out and do it right."[65]

Effects on clinical care
The complexity of HIPAA, combined with potentially stiff penalties for violators, can lead physicians and medical centers to withhold information from those who may have a right to it. A review of the implementation of the HIPAA Privacy Rule by the U.S. Government Accountability Office found that health care providers were "uncertain about their legal privacy responsibilities and often responded with an overly guarded approach to disclosing information ... than necessary to ensure compliance with the Privacy rule".[65] Reports of this uncertainty continue.[68]

Costs of implementation
In the period immediately before the enactment of the HIPAA Privacy and Security Acts, medical centers and medical practices were charged with complying with the new requirements. Many practices and centers turned to private consultants for compliance assistance.[citation needed]

Education and training
Education and training of healthcare providers is a requirement for correct implementation of both the HIPAA Privacy Rule and Security Rule.[69][70]


Misspelling as HIPPA
"People make up what that acronym stands for."

Deven McGraw, former HHS deputy director, as quoted in Vox[71]

Although the acronym HIPAA matches the title of the 1996 Public Law 104-191, Health Insurance Portability and Accountability Act, HIPAA is sometimes incorrectly referred to as HIPPA, variously said to refer to the "Health Information Privacy and Portability Act",[72][73] "Health Information Privacy Protection Act",[71] or "Health Insurance Privacy and Protection Act".[74] The HIPPA misspelling has been observed among COVID-19 scammers.[75]

Violations
HIPAA Chart illustrating HIPAA violations by Type
A breakdown of the HIPAA violations that resulted in the illegal exposure of personal information.
According to the US Department of Health and Human Services Office for Civil Rights, between April 2003 and January 2013, it received 91,000 complaints of HIPAA violations, in which 22,000 led to enforcement actions of varying kinds (from settlements to fines) and 521 led to referrals to the US Department of Justice as criminal actions.[76] Examples of significant breaches of protected information and other HIPAA violations include:

The largest loss of data that affected 4.9 million people by Tricare Management of Virginia in 2011[77]
The largest fines of $5.5 million levied against Memorial Healthcare Systems in 2017 for accessing confidential information of 115,143 patients and of $4.3 million levied against Cignet Health of Maryland in 2010 for ignoring patients' requests to obtain copies of their own records and repeated ignoring of federal officials' inquiries[78]
The first criminal indictment was lodged in 2011 against a Virginia physician who shared information with a patient's employer "under the false pretenses that the patient was a serious and imminent threat to the safety of the public, when in fact he knew that the patient was not such a threat."[citation needed]
According to Koczkodaj et al., 2018,[79] the total number of individuals affected since October 2009 is 173,398,820.

The differences between civil and criminal penalties are summarized in the following table:

Type of Violation	CIVIL Penalty (min)	CIVIL Penalty (max)
Individual did not know (and by exercising reasonable diligence would not have known) that he/she violated HIPAA	$100 per violation, with an annual maximum of $25,000 for repeat violations	$50,000 per violation, with an annual maximum of $1.5 million
HIPAA violation due to reasonable cause and not due to willful neglect	$1,000 per violation, with an annual maximum of $100,000 for repeat violations	$50,000 per violation, with an annual maximum of $1.5 million
HIPAA violation due to willful neglect but violation is corrected within the required time period	$10,000 per violation, with an annual maximum of $250,000 for repeat violations	$50,000 per violation, with an annual maximum of $1.5 million
HIPAA violation is due to willful neglect and is not corrected	$50,000 per violation, with an annual maximum of $1,000,000	$50,000 per violation, with an annual maximum of $1.5 million
Type of Violation	CRIMINAL Penalty
Covered entities and specified individuals who "knowingly" obtain or disclose individually identifiable health information	A fine of up to $50,000
Imprisonment up to 1 year

Offenses committed under false pretenses	A fine of up to $100,000
Imprisonment up to 5 years

Offenses committed with the intent to sell, transfer, or use individually identifiable health information for commercial advantage, personal gain or malicious harm	A fine of up to $250,000
Imprisonment up to 10 years

Legislative information
In 1994, President Clinton expressed his goals to improve the healthcare system. However, his reforms did not succeed, most likely due to lack of support.[80] The Congressional Quarterly Almanac of 1996 explains how two senators, Nancy Kassebaum (R-KS) and Ted Kennedy (D-MA) came together and created a bill called the Health Insurance Reform Act of 1995 or more commonly known as the Kassebaum-Kennedy Bill.[81] This bill was stalled despite making it out of the Senate. In the 1996 State of the Union address, Clinton pressed the issue, and it resulted in bipartisan cooperation.[80] After much debate and negotiation, there was a shift in momentum once a compromise between Kennedy and Ways and Means Committee Chairman Bill Archer was accepted, after alterations were made of the original Kassebaum-Kennedy Bill.[82] Soon after this, the bill was signed into law by President Clinton and was named the Health Insurance Portability and Accountability Act of 1996 (HIPAA).

Pub. L.Tooltip Public Law (United States) 104–191 (text) (PDF), 110 Stat. 1936
H.R. 3103; H. Rept. 104–469, part 1; H. Rept. 104-736
S. 1028; S. 1698; S. Rept. 104-156
HHS Security Standards, 45 CFR 160, 45 CFR 162, and 45 CFR 164
HHS Standards for Privacy of Individually Identifiable Health Information, 45 CFR 160 and 45 CFR 164
References
 Atchinson, Brian K.; Fox, Daniel M. (May–June 1997). "The Politics Of The Health Insurance Portability And Accountability Act" (PDF). Health Affairs. 16 (3): 146–150. doi:10.1377/hlthaff.16.3.146. PMID 9141331. Archived from the original (PDF) on 2014-01-16. Retrieved 2014-01-16.
 "104th Congress, 1st Session, S.1028" (PDF). Archived (PDF) from the original on 2012-06-16.
 "Health insurance portability and accountability act of 1996". Public Law.
 Edemekong, Peter F.; Annamaraju, Pavan; Haydel, Micelle J. (2023), "Health Insurance Portability and Accountability Act", StatPearls, Treasure Island (FL): StatPearls Publishing, PMID 29763195, retrieved 2023-06-15
 "Your Medical Records". 19 November 2008.
 "Health Plans & Benefits: Portability of Health Coverage". United States Department of Labor. 2015-12-09. Archived from the original on 2016-12-20. Retrieved 2016-11-05.
 "Overview". www.cms.gov. 2016-09-13. Archived from the original on 2016-11-02. Retrieved 2016-11-05.
 Berger, Mark C.; Black, Dan A.; Scott, Frank A. (2004). "Is There Job Lock? Evidence from the Pre-HIPAA Era". Southern Economic Journal. 70 (4): 953–976. doi:10.2307/4135282. ISSN 0038-4038. JSTOR 4135282.
 "HIPAA Title Information". www.dhcs.ca.gov. Retrieved 2021-10-31.
 29 U.S.C. § 1181(a)(2)
 29 U.S.C. § 1181(a)(3)
 29 U.S.C. § 1181(c)(1)
 29 U.S.C. § 1181(c)(2)(A)
 (Sub B Sec 111)
 "HIPAA for Healthcare Workers: The Privacy Rule". 2014. doi:10.4135/9781529727890. {{cite journal}}: Cite journal requires |journal= (help)
 42 U.S.C. § 1320a-7c
 42 U.S.C. § 1395ddd
 42 U.S.C. § 1395b-5
 "42 U.S. Code § 1395ddd - Medicare Integrity Program". LII / Legal Information Institute. Archived from the original on 2018-03-21. Retrieved 2018-03-21.
 45 CFR 160.103
 "Other Administrative Simplification Rules". 4 December 2015.
 Terry, Ken "Patient Privacy - The New Threats" Archived 2015-11-20 at the Wayback Machine Physicians Practice journal, volume 19, number 3, year 2009, access date July 2, 2009
 See 45 CFR Sections 160.102 and 160.103 Archived 2012-01-12 at the Wayback Machine.
 45 CFR 164.524
 45 CFR 164.512
 (OCR), Office for Civil Rights (7 May 2008). "Summary of the HIPAA Privacy Rule". Archived from the original on 6 December 2015.
 45 CFR 164.524
 45 CFR 164.502
 45 CFR 164.502
 45 CFR 164.526
 45 CFR 164.522
 Rowe, Linda (2005). "What Judicial Officers Need to Know about the HIPAA Privacy Rule". NASPA Journal. 42 (4): 498–512. doi:10.2202/0027-6014.1537. ProQuest 62084860.
 45 CFR 164.528
 45 CFR 164.530
 45 CFR 164.530
 "How to File A Health Information Privacy Complaint with the Office for Civil Rights" (PDF). Archived from the original (PDF) on 2016-12-21. Retrieved 2017-10-07.
 45 CFR 160.306
 "Spread of records stirs fears of privacy erosion" Archived 2017-07-10 at the Wayback Machine, December 23, 2006, by Theo Francis, The Wall Street Journal
 "University of California settles HIPAA Privacy and Security case involving UCLA Health System facilities". Department of Health and Human Services. Archived from the original on 2017-10-12.
 Kavi, Aishvarya (2021-07-22). "How the HIPAA Law Works and Why People Get It Wrong". The New York Times. ISSN 0362-4331. Retrieved 2021-07-23.
 Chiu, Allyson (2021-05-22). "Explaining HIPAA: No, it doesn't ban questions about your vaccination status". Washington Post. Retrieved 2021-07-23.
 "Lawmaker Marjorie Taylor Greene, in Ten Words or Less, Gets HIPAA All Wrong". Law & Crime. 2021-07-21. Retrieved 2021-07-23.
 (OCR), Office for Civil Rights (30 October 2015). "Omnibus HIPAA Rulemaking".
 Health Information of Deceased Individuals Archived 2017-10-19 at the Wayback Machine 2013
 Cohen, Jessica Kim (2017-08-30). "HHS releases limited HIPAA waiver during Hurricane Harvey: 5 things to know". www.beckershospitalreview.com. Retrieved 2024-03-19.
 (OCR), Health Information Privacy Division, Office for Civil Rights (2016-01-05). "Individuals' Right under HIPAA to Access their Health Information". HHS.gov. Archived from the original on 2017-12-02. Retrieved 2017-12-10. {{cite news}}: |first= has generic name (help)
 Rights (OCR), Office for Civil (2016-06-24). "2042-What personal health information do individuals have a right under HIPAA to access from their health care providers and health plans?". HHS.gov. Retrieved 2021-09-01.
 "Individuals' Right under HIPAA to Access their Health Information 45 CFR § 164.524". U.S. Department of Health & Human Services. 5 January 2016. Retrieved 10 April 2021.
 Rights (OCR), Office for Civil (2009-11-20). "Summary of the HIPAA Security Rule". HHS.gov. Retrieved 2021-03-17.
 Ahlers, Mike M. (25 February 2014). "Asiana fined $500,000 for failing to help families - CNN". CNN. Archived from the original on 2014-02-27.
 "First Amendment Center | Freedom Forum Institute". Archived from the original on 2016-06-05. Retrieved 2016-04-19.
 "New York Times Examines 'Unintended Consequences' of HIPAA Privacy Rule". 3 June 2003. Archived from the original on 6 May 2016.
 U.S. Social Security Administration. "TITLE XI—General Provisions, Peer Review, and Administrative Simplification". www.ssa.gov. Retrieved 2020-07-18.
 Traynor, Kate (2002). "HIPAA compliance date for electronic transactions extended". American Journal of Health-System Pharmacy. 59 (5): 402. doi:10.1093/ajhp/59.5.402. PMID 11887402. Retrieved 2023-12-16.
 "Overview". www.cms.gov. 26 July 2017. Archived from the original on 18 October 2017.
 "Overview". www.cms.gov. 28 March 2016. Archived from the original on 12 February 2012.
 CSM.gov "Medicare & Medicaid Services" "Standards for Electronic Transactions-New Versions, New Standard and New Code Set – Final Rules"
 "The Looming Problem in Healthcare EDI: ICD-10 and HIPAA 5010 migration" October 10, 2009 – Shahid N. Shah
 "HIPAA security rule & risk analysis". American Medical Association. 14 December 2023.
 Wafa, Tim (Summer 2010). "How the Lack of Prescriptive Technical Granularity in HIPAA Has Compromised Patient Privacy". Northern Illinois University Law Review. 30 (3). SSRN 1547425.
 Health Insurance Portability and Accountability Act of 1996 (HIPAA). Archived 2014-01-08 at the Wayback Machine Steve Anderson: HealthInsurance.org.
 Medical Privacy Law Nets No Fines. Archived 2017-10-13 at the Wayback Machine Rob Stein: The Washington Post.
 "Feds step up HIPAA enforcement with hospice settlement - SC Magazine". 7 January 2013. Archived from the original on 2013-01-09. Retrieved 2013-01-09. Feds step up HIPAA enforcement with hospice settlement
 Kirsch, Michael S. (2004). "Alternative Sanctions and the Federal Tax Law: Symbols, Shaming, and Social Norm Management as a Substitute for Effective Tax Policy". Iowa Law Review. 89 (863). SSRN 552730.
 Wilson J (2006). "Health Insurance Portability and Accountability Act Privacy rule causes ongoing concerns among clinicians and researchers". Ann Intern Med. 145 (4): 313–6. doi:10.7326/0003-4819-145-4-200608150-00019. PMID 16908928. S2CID 32140125.
 Armstrong D, Kline-Rogers E, Jani S, Goldman E, Fang J, Mukherjee D, Nallamothu B, Eagle K (2005). "Potential impact of the HIPAA privacy rule on data collection in a registry of patients with acute coronary syndrome". Arch Intern Med. 165 (10): 1125–9. doi:10.1001/archinte.165.10.1125. PMID 15911725.
 Wolf M, Bennett C (2006). "Local perspective of the impact of the HIPAA privacy rule on research". Cancer. 106 (2): 474–9. doi:10.1002/cncr.21599. PMID 16342254.
 Gross, Jane (July 3, 2007). "Keeping Patients' Details Private, Even From Kin". The New York Times. Archived from the original on August 12, 2017. Retrieved August 11, 2019.
 "Federal Register :: Request Access".
 "Federal Register :: Request Access".
 Morrison, Sara (20 April 2021). "HIPAA, the health privacy law that's more limited than you think, explained". Vox. Retrieved 31 October 2023.
 "United States District Court" (PDF). September 16, 2010. violation of the Health Information Privacy and Portability Act. ("HIPPA")
 S. E. Ross (2003). "The Effects of Promoting Patient Access to Medical Records: A Review". Journal of the American Medical Informatics Association. 10 (2): 129–138. doi:10.1197/jamia.M1147. PMC 150366. PMID 12595402. The Health Insurance Privacy and Portability Act (HIPPA) stipulates that ...
 "No, asking if you are vaccinated is not a HIPAA violation". wtsp.com. September 13, 2021. Retrieved 31 October 2023.
 LaFraniere, Sharon; Hamby, Chris (5 April 2020). "Another Thing to Fear Out There: Coronavirus Scammers". The New York Times. Retrieved 31 October 2023.
 "Enforcement Highlights". OCR Home, Health Information Privacy, Enforcement Activities & Results, Enforcement Highlights. U.S. Department of Health & Human Services. Archived from the original on 5 March 2014. Retrieved 3 March 2014.
 "Breaches Affecting 500 or more Individuals". OCR Home, Health Information Privacy, HIPAA Administrative Simplification Statute and Rules, Breach Notification Rule. U.S. Department of Health & Human Services. Archived from the original on 15 March 2015. Retrieved 3 March 2014.
 "Civil Money Penalty". HHS Official Site. U.S. Department of Health & Human Services. October 2010. Archived from the original on 8 October 2017. Retrieved 8 October 2017.
 Koczkodaj, Waldemar W.; Mazurek, Mirosław; Strzałka, Dominik; Wolny-Dominiak, Alicja; Woodbury-Smith, Marc (2019-01-01). "Electronic Health Record Breaches as Social Indicators". Social Indicators Research. 141 (2): 861–871. doi:10.1007/s11205-018-1837-z. ISSN 1573-0921. S2CID 255006896.
 "Health Insurance Portability and Accountability Act - LIMSWiki". www.limswiki.org. Retrieved 2021-10-10.
 Cade, Dozier C. (1951). "Book Review: Congressional Quarterly Almanac: 81st Congress, 2nd Session. Vol. VI". Journalism Quarterly. 28 (3): 389–390. doi:10.1177/107769905102800313. ISSN 0022-5533. S2CID 164443756.
 "The Health Insurance Portability and Accountability Act (HIPAA) | Colleaga". www.colleaga.org. Retrieved 2021-10-10.[dead link]
External links
California Office of HIPAA Implementation Archived 2012-11-01 at the Wayback Machine (CalOHI)
"HIPAA", Centers for Medicare and Medicaid Services (CMS)
Congressional Research Service (CRS) reports regarding HIPAA, University of North Texas Libraries
Full text of the Health Insurance Portability and Accountability Act (PDF/TXT) U.S. Government Printing Office
Office for Civil Rights page on HIPAA
vte
Insurance
vte
Telemedicine and telehealth
Authority control databases Edit this at Wikidata
Categories: Acts of the 104th United States CongressData erasureInsurance legislationMedical privacy legislationMedicare and Medicaid (United States)Privacy law in the United StatesSecurity complianceUnited States federal health legislationUnited States federal privacy legislationUnited States federal insurance legislation
==========================================================
Children's Online Privacy Protection Act

Article
Talk
Read
Edit
View history

Tools
From Wikipedia, the free encyclopedia
For the never-enacted law passed in 1998, see Child Online Protection Act. For the proposed anti-pornography exposure law, see Children's Internet Protection Act.
"COPPA" redirects here. For the California privacy act, see CalOPPA. For the association of disability rights attorneys and parents, see COPAA. For other uses, see Coppa.

This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.
Find sources: "Children's Online Privacy Protection Act" – news · newspapers · books · scholar · JSTOR (January 2024) (Learn how and when to remove this message)
Children's Online Privacy Protection Act
Great Seal of the United States
Acronyms (colloquial)	COPPA
Enacted by	the 105th United States Congress
Effective	April 21, 2000; 24 years ago
Citations
Public law	105-277
Legislative history
Introduced in the Senate as S. 2326 by Richard Bryan (D–NV) on July 17, 1998
Committee consideration by Senate Commerce, Science, and Transportation
Signed into law by President Bill Clinton on October 21, 1998
Youth rights
Activities
Theory/concepts
Issues
Organizations
People
Related
 Youth rightsicon Society portal
vte
The Children's Online Privacy Protection Act of 1998 (COPPA) is a United States federal law, located at 15 U.S.C. §§ 6501–6506 (Pub. L.Tooltip Public Law (United States) 105–277 (text) (PDF), 112 Stat. 2681-728, enacted October 21, 1998).

The act, effective April 21, 2000, applies to the online collection of personal information by persons or entities under U.S. jurisdiction about children under 13 years of age, including children outside the U.S. if the website or service is U.S.-based.[1] It details what a website operator must include in a privacy policy, when and how to seek verifiable consent from a parent or guardian, and what responsibilities an operator has to protect children's privacy and safety online, including restrictions on the marketing of those under 13.[2]

Although children under 13 can legally give out personal information with their parents' permission, many websites—particularly social media sites, but also other sites that collect most personal info—disallow children under 13 from using their services altogether due to the cost and work involved in complying with the law.[3][4][5]

An updated version of COPPA, the Children and Teens' Online Privacy Protection Act, informally called COPPA 2.0, has been introduced in the 118th Congress in 2023, effectively raising the age covered by COPPA from 13 to 16 years.[6]

Background
In the 1990s, electronic commerce was on its rise of popularity, but various concerns were expressed about the data collection practices and the impact of Internet commerce on user privacy—especially for children under 13, because very few websites had their own privacy policies.[7] The Center for Media Education petitioned the Federal Trade Commission (FTC) to investigate the data collection and use practices of the KidsCom website, and take legal action since the data practices violated Section 5 of FTC Act concerning "unfair/deceptive practices." With the passing of the Drivers Privacy Protection Act in 1997, new precedents had been set in regard to the ability of congress to regulate information held by state agencies.[8] After the FTC completed its investigation, it issued the "KidsCom Letter" the report stated that the data collection and use practices were indeed subject to legal action.[9][10] This resulted in the need to inform parents about the risks of children's online privacy, as well as to parental consent necessity. This ultimately resulted in the drafting of COPPA.

The new millennium ushered in an era of regulation that many were simply unaware of. The early years of the transition were fraught with confusion and a lot of animosity. One of the main concerns of the time was the eventual accessibility of child-based websites at the fear many were unwilling to change their business practices.[11] Many were left with a series of loose guidelines that determined what was correct.[12] The simplification of COPPA provided by the FTC was met with a follow-up of demands to law enforcement that the: "... Commission should continue law enforcement efforts by targeting significant violations and seeking increasingly larger civil penalties, when appropriate, to deter unlawful conduct".[13] A mandatory review of the COPPA regulations were conducted in 2005 (resulting with no changes to the original guidelines), found that there were no adverse effects to the online landscape.

The Federal Trade Commission (FTC) has the authority to issue regulations and enforce COPPA. Also, under the terms of COPPA, the FTC-designated "safe harbor" provisioning is designed to encourage increased industrial self-regulation. Under this provision, industry groups and others may request Commission approval of self-regulatory guidelines to govern participants' compliance, such that website operators in Commission-approved programs would first be subject to the disciplinary procedures of the safe harbor program in lieu of FTC enforcement. As of June 2016, the FTC has approved seven safe harbor programs operated by TrustArc, ESRB, CARU, PRIVO, Aristotle, Inc., Samet Privacy (kidSAFE), and the Internet Keep Safe Coalition (iKeepSafe).[14][15] In August 2021, Aristotle, Inc. withdrew from the safe harbor program after FTC staff expressed serious concerns about its enforcement of its safe harbor provisions and communicated their intent to recommend the revocation of Aristotle's approval to run a safe harbor program. The FTC also announced its intention to more closely scrutinize the practices of the other six present safe harbors.[16]

In September 2011, the FTC announced proposed revisions to the COPPA rules, the first significant changes to the act since the issuance of the rules in 2000. The proposed rule changes expanded the definition of what it meant to "collect" data from children. The proposed rules presented a data retention and deletion requirement, which mandated that data obtained from children be retained only for the amount of time necessary to achieve the purpose that it was collected for. It also added the requirement that operators ensure that any third parties to whom a child's information is disclosed have reasonable procedures in place to protect the information.[17]

The act applies to websites and online services operated for commercial purposes that are either directed toward children under 13 or have actual knowledge that children under 13 are providing information online. Most recognized non-profit organizations are exempt from most of the requirements of COPPA.[2] However, the Supreme Court ruled that non-profits operated for the benefit of their members' commercial activities are subject to FTC regulation and consequently COPPA as well.[18] The type of "verifiable parental consent" that is required before collecting and using information provided by children under 13 is based upon a "sliding scale" set forth in a Federal Trade Commission regulation[19] that takes into account the manner in which the information is being collected and the uses to which the information will be put.

Violations
According to the FTC, courts may fine violators of COPPA up to $50,120 in civil penalties for each violation.[20] The FTC has brought a number of actions against website operators for failing to comply with COPPA requirements, including actions against Google, TikTok, Girls' Life,[21] American Pop Corn Company,[22] Lisa Frank, Inc.,[23] Mrs. Fields Cookies, and The Hershey Company.[24]

In February 2004, UMG Recordings, Inc. was fined US$400,000 for COPPA violations in connection with a website that promoted the then 13-year-old rapper Lil' Romeo and hosted child-oriented games and activities, and Bonzi Software, which offered downloads of an animated figure "BonziBuddy" that provided shopping advice, jokes, and trivia was fined $75,000 for COPPA violations.[25] Similarly, the owners of the Xanga website were fined US$1,000,000 in 2006 for COPPA violations of repeatedly allowing children under 13 to sign up for the service without getting their parent's consent.[26]

In 2016, the mobile advertising network inMobi was fined US$950,000 for tracking the geo-location of all users (including those under 12) without their knowledge. The advertising software continuously tracked user location despite privacy preferences on the mobile device.[27] Other websites that were directed towards children and fined due to COPPA include Imbee (2008),[28] Kidswirl (2011)[29] and Skid-e-Kids (2011).[30]

In February 2019, the FTC issued a fine of $5.7 million to ByteDance for failing to comply with COPPA with their TikTok app (then called Musical.ly). ByteDance agreed to pay the largest COPPA fine since the bill's enactment and to add a kids-only mode to the TikTok app.[31]

Three dating apps by Wildec were pulled by Apple and Google from their respective app stores, after the FTC determined that the dating apps allowed users under 13 to register, that Wildec knew there were significant numbers of minor users, and that this allowed inappropriate contact with minors.[32]

See also: YouTube and privacy § COPPA settlement
On September 4, 2019, the FTC issued a fine of $170 million to YouTube for COPPA violations, including tracking the viewing history of minors in order to facilitate targeted advertising.[33] Many notable social media platforms were subjected to scrutiny from the FTC, especially groups like Facebook where the platform had users ignoring COPPA guidelines since inception.[34] As a result, YouTube announced that as part of the settlement, in 2020 it would require channel operators to mark videos that are "child-oriented" as such, and would use machine learning to automatically mark those as clearly "child-oriented" if not marked already. In the settlement terms, channel operators that failed to mark videos as "child-oriented" could be fined by the FTC for up to $42,530 per video,[35] which has raised criticism towards the settlement terms.[36][37] The decision came in terms that, despite good faith, created many issues among the content creators on the site. Users such as Ryan's World, Philip DeFranco and TheOdd1sOut with vastly different content found themselves in the hot seat for their appealing content for children.[38] The following guidelines were implemented on the basis set by the following rules:

The Rule sets out additional factors the FTC will consider in determining whether your content is child-directed:

the subject matter,
visual content,
the use of animated characters or child-oriented activities and incentives,
the kind of music or other audio content,
the age of models,
the presence of child celebrities or celebrities who appeal to children,
language or other characteristics of the site,
whether advertising that promotes or appears on the site is directed to children, and
competent and reliable empirical evidence about the age of the audience.[39]
In 2022, Epic Games settled a Federal Trade Commission complaint in part by agreeing to pay a $275 million penalty for COPPA violations. The FTC complaint alleged that Epic illegally collected personal information from children under the age of 13 and made it difficult for parents to get such information deleted. The full agreement included an additional $245 million to refund users who were manipulated into making unintended purchases.[40]

Compliance
In December 2012, the Federal Trade Commission issued revisions effective July 1, 2013, which created additional parental notice and consent requirements, amended definitions, and added other obligations for organizations that (1) operate a website or online service that is "directed to children" under 12 and that collects "personal information" from users or (2) knowingly collects personal information from people under 13 through a website or online service.[41] After July 1, 2013, operators must:[42]

Post a clear and comprehensive online privacy policy describing their information practices for personal information collected online from persons under age 13;
Make reasonable efforts (taking into account available technology) to provide direct notice to parents of the operator's practices with regard to the collection, use, or disclosure of personal information from persons under 13, including notice of any material change to such practices to which the parents have previously consented;
Obtain verifiable parental consent, with limited exceptions, prior to any collection, use, and/or disclosure of personal information from persons under age 13;
Provide a reasonable means for a parent to review the personal information collected from their child and to refuse to permit its further use or maintenance;
Establish and maintain reasonable procedures to protect the confidentiality, security, and integrity of the personal information collected from children under age 13, including by taking reasonable steps to disclose/release such personal information only to parties capable of maintaining its confidentiality and security; and
Retain personal information collected online from a child for only as long as is necessary to fulfill the purpose for which it was collected and delete the information using reasonable measures to protect against its unauthorized access or use.
Operators are prohibited from conditioning a child's participation in an online activity on the child providing more information than is reasonably necessary to participate in that activity.[43]
According to a notice issued by the Federal Trade Commission, an operator has actual knowledge of a user's age if the site or service asks for—and receives—information from the user that allows it to determine the person's age.[44] An example, cited by the FTC, includes an operator who asks for a date of birth on a site's registration page has actual knowledge as defined by COPPA if a user responds with a year that suggests they are under 13. Another example cited by the FTC is that an operator may have actual knowledge based on answers to "age identifying" questions like "What grade are you in?" or "What type of school do you go to? (a) elementary; (b) middle; (c) high school; (d) college."

A small fee was charged by Microsoft under COPPA as a way to verify parental consent. The fee was donated to the National Center for Missing and Exploited Children.[45] Google, however, charges a small fee as a way to verify one's date of birth.

In the changes effective July 1, 2013, the definition of an operator was updated to make clear that COPPA covers a child-directed site or service that integrates outside services, such as plug-ins or advertising networks, that collect personal information from its visitors.[46] The definition of a website or online service directed to children is expanded to include plug-ins or ad networks that have actual knowledge that they are collecting personal information through a child-directed website or online service. Websites and services that target children as a secondary audience may differentiate among users, and are required to provide notice and obtain parental consent only for those users who identify themselves as being younger than 13.[41] The definition of personal information requiring parental notice and consent before collection now includes "persistent identifiers" that can be used to recognize users over time and across different websites or online services. However, no parental notice and consent are required when an operator collects a persistent identifier for the sole purpose of supporting the website or online service's internal operations.[46] The definition of personal information after July 1, 2013, also includes geolocation information, as well as photos, videos, and audio files that contain a child's image or voice.[42]

On November 19, 2015, the FTC announced it had approved an additional method for obtaining verifiable parental consent: "face match to verified photo identification" (FMVPI). The two-step process allows a parent to submit a government-sanctioned ID for authentication, then submit an impromptu photo via mobile device or web camera, which is then compared to the photo on the ID.[47]

International scope
The FTC has asserted that COPPA applies to any online service that is directed to U.S. users or knowingly collects information from children in the U.S., regardless of its country of origin. Referring to their official website, the following embodies such views:

The FTC's Office of International Affairs directs the agency's international activities for competition and consumer protection, which include:

strengthening relationships with foreign competition and consumer protection agencies
developing formal and informal arrangements and agreements with competition and consumer protection agencies around the world
engaging in cooperative dialogues and submitting reports at international forums for competition and consumer protection
helping agencies around the world develop and enhance their own competition and consumer protection programs
sharing information with foreign law enforcement authorities through the U.S. Safe Web Act
maintaining a robust International Fellows Program[48]
However, the FTC rarely performs enforcement actions against foreign companies, and faces a number of practical challenges in doing so.[49] The general assumption is that, despite the interconnected world of internet services, jurisdiction only applies to domestic operation. Nevertheless, it has successfully enforced COPPA against at least one foreign company with a significant US userbase, securing a $5.7 million settlement against the Chinese company ByteDance over their TikTok app.[50]

Criticisms
See also: YouTube and privacy § COPPA settlement
COPPA is controversial and has been criticized as ineffective and potentially unconstitutional by legal experts[51] and mass media[52] since it was drafted.[53] Complaints leveled against the legislation include website owners banning users 12 and under—which only "encourages age fraud and allows websites to bypass the burden of obtaining parental consent"[51]—and the active suppression of children's rights to freedom of speech, self-expression, and other First Amendment rights[54][55] due to necessity of registering accounts to do so.

Delays in obtaining parental consent often result in children moving on to other activities that are less appropriate for their age or pose bigger privacy risks.[56]

In addition, age restrictions and the "parental consent" process are easy for children to circumvent, and parents generally help them to lie about their age.[57][58]

An Internet Safety Technical Task Force composed of experts from academia and commercial companies found in 2012 that mandatory age verification is not only a poor solution for privacy but also constitutes a violation of privacy.[59] The law has also many safety flaws. For example, it does not protect kids from predatory advertising,[60] it does not prevent kids from accessing pornography or lying about their age,[2] and it does not ensure a totally safe environment online. Tech journalist Larry Magid, a long-time vocal opponent of the law,[52][54][7] also notes that parents, not the government, hold the bulk of responsibility of protecting children online.[7] COPPA has also been criticized for its potential chilling effect on children's apps, content, websites and online services. For example, Snapchat released a Snapkidz version of its app in June 2013, but unlike Snapchat, Snapkidz did not allow photo sharing at all due to COPPA regulations.[61] Similarly, it has been pointed out that the COPPA Rule was not necessarily about privacy protection but more about "enforcing the laws."[56]

COPPA's penalties ($40,000 per violation) can be potentially catastrophic for small businesses, undermining their business model.[62][63] By contrast, the FTC has been criticized, including by COPPA author Ed Markey, and FTC commissioner Rohit Chopra, for not fining major and big tech companies harshly enough for their COPPA violations, especially in comparison to their revenue. In contrast, violators of the European Union's General Data Protection Regulation (GDPR) may be fined up to 4% of their annual global revenue.[64][65][66]

With the rise of virtual education, COPPA may inadequately represent the role of administrators, teachers, and the school in protecting student privacy under the assumption of loco parentis.[67]

Mark Zuckerberg, co-founder and CEO of Facebook, expressed opposition to COPPA in 2011 and stated "That will be a fight we take on at some point. My philosophy is that for education you need to start at a really, really young age."[68] The next year, Jim Steyer, the CEO of Common Sense Media, has called for updates to COPPA, calling the time of the act's creation "the stone age of digital media" and pointing out the lack of platforms such as Google, YouTube, Facebook and Twitter at the time.[69]

In 2019, the Government of the State of New York sued YouTube for violating COPPA by illegally retaining information related to children under 13 years of age. YouTube responded by dividing its content strictly into "for kids" and "not for kids". This has met with extremely harsh criticism from the YouTube community, especially from gamers, with many alleging that the FTC of the United States intends to fine content creators $42,530 for "each mislabeled video", possibly putting all users at risk.[70][71][72] However, some have expressed skepticism over this, feeling that the fines may actually be in reference to civil penalties, possibly intended for the site's operators and/or warranted by more serious of COPPA violations or specific cases of "mislabeling videos".[73][74][75] As of December 2022, no YouTuber has been fined.[76]

Several bills have been proposed to amend COPPA. Markey and Josh Hawley introduced multiple bills (in the House in 2018 as the "Do Not Track Kids Act", and in 2019 as a Senate measure) proposing that COPPA ban the use of targeted advertising to users under 13, require personal consent before the collection of personal information from users ages 13–15, require connected devices and toys directed towards children to meet security standards and include a privacy policy disclosure on their packaging, and require services to offer an "eraser button" to "permit users to eliminate publicly available personal information content submitted by the child, when technologically feasible". In January 2020, Bobby Rush and Tim Walberg introduced a similar house bill known as the Preventing Real Online Threats Endangering Children Today (PROTECT Kids) Act, which would extend all existing COPPA consent requirements to users under the age of 16, and explicitly add mobile apps, "precise geolocation", and biometric data to its remit.[77][78][79]

See also
Adultism
Child Online Protection Act (COPA)
Child Protection Registry Acts
Do Not Track legislation
General Data Protection Regulation
Kids Online Safety Act (KOSA)
California Online Privacy Protection Act (OPPA) effective since July 1, 2004
Health Insurance Portability and Accountability Act
References
 "Complying with COPPA: Frequently Asked Questions". FTC Business Center. Federal Trade Commission. March 20, 2015. Retrieved August 6, 2019. As a related matter, U.S.-based sites and services that collect information from foreign children also are subject to COPPA.
 "Complying with COPPA: Frequently Asked Questions". FTC Business Center. Federal Trade Commission. March 20, 2015. Retrieved August 6, 2019.
 "What age should my kids be before I let them use Instagram, Facebook, and other social media services?". Common Sense Media. Common Sense Media, Inc. Archived from the original on August 6, 2018. Retrieved April 29, 2023.
 Bilton, N. (February 18, 2015). "Letting Your Kids Play in the Social Media Sandbox". The New York Times. Archived from the original on February 22, 2015. Retrieved July 21, 2019.
 Rochman, B. (May 24, 2011). "Should Kids Under 13 Be on Facebook?". Time. Time, Inc. Retrieved June 22, 2016.
 Fingas, Jon (May 2, 2023). "Senators reintroduce COPPA 2.0 bill to tighten child safety online". Engadget. Retrieved February 18, 2024.
 Magid, L.J. (April 24, 2000). "New Law Protects Kids Online, but It's No Substitute for Parenting". Los Angeles Times. Archived from the original on December 22, 2015. Retrieved June 22, 2016.
 "Redirecting..." heinonline.org. Retrieved March 26, 2022. {{cite web}}: Cite uses generic title (help)
 Warmund, J. (2001). "Can COPPA Work? An Analysis of the Parental Consent Measures in the Children's Online Privacy Protection Act". Fordham Intellectual Property, Media, and Entertainment Law Journal. 11 (1). Retrieved June 22, 2016.
 "FTC Staff Sets Forth Principles For Online Information Collection From Children". FTC Press Releases. Federal Trade Commission. July 16, 1997. Retrieved June 22, 2016.
 Davis, Joel J. (Autumn 2002). "Marketing to children online: A manager's guide to the Children's Online Privacy Protection Act". S.A.M. Advanced Management Journal. 67: 11–63. ProQuest 231236782.
 Reyes, Irwin (2018). ""Won't Somebody Think of the Children?" Examining COPPA Compliance at Scale" (PDF). Proceedings on Privacy Enhancing Technologies. 2018 (3): 63–83. doi:10.1515/popets-2018-0021. S2CID 4935390.
 Commission., United States. Federal Trade (2007). Implementing the Children's Online Privacy Protection Act : a report to Congress. U.S. FTC. OCLC 85854528.
 Thomas, L.M. (August 19, 2014). "FTC Approves iKeepSafe's COPPA Safe Harbor Program". Privacy Law Corner. Winston & Strawn LLP. Archived from the original on September 21, 2016. Retrieved June 22, 2016.
 Thomas, L.M. (February 20, 2014). "FTC Approves Sixth COPPA Safe Harbor Program". Privacy Law Corner. Winston & Strawn LLP. Retrieved June 22, 2016.
 "Aristotle Removed from List of FTC-Approved Children's Privacy Self-Regulatory Programs". FTC Press Releases. Federal Trade Commission. August 4, 2021. Retrieved September 2, 2021.
 "FTC Will Propose Broader Children's Online Privacy Safeguards". The National Law Review. Ifrah PLLC. December 22, 2011. Retrieved June 22, 2016.
 "FTC v. California Dental Association, 526 U.S. 756 (1999)". Justia. May 24, 1999. Retrieved June 22, 2016.
 Federal Trade Commission (November 3, 1999). "16 CFR Part 312 Children's Online Privacy Protection Rule; Final Rule" (PDF). Federal Register. 64 (212): 59888–59915. Archived from the original (PDF) on October 29, 2013. Retrieved June 22, 2016.
 "Complying with COPPA: Frequently Asked Questions". Federal Trade Commission. July 20, 2020. Retrieved October 25, 2020.
 "FTC Announces Settlements with Web Sites That Collected Children's Personal Data Without Parental Permission". FTC Press Releases. Federal Trade Commission. April 19, 2001. Retrieved June 22, 2016.
 "Popcorn Company Settles FTC Privacy Violation Charges". FTC Press Releases. Federal Trade Commission. February 14, 2002. Retrieved June 22, 2016.
 "Web Site Targeting Girls Settles FTC Privacy Charges". FTC Press Releases. Federal Trade Commission. October 2, 2001. Retrieved June 22, 2016.
 "FTC Receives Largest COPPA Civil Penalties to Date in Settlements with Mrs. Fields Cookies and Hershey Foods". FTC Press Releases. Federal Trade Commission. February 27, 2003. Retrieved June 22, 2016.
 "UMG Recordings, Inc. to Pay $400,000, Bonzi Software, Inc. To Pay $75,000 to Settle COPPA Civil Penalty Charges". FTC Press Releases. Federal Trade Commission. February 18, 2004. Retrieved June 22, 2016.
 Sullivan, B. (September 7, 2006). "FTC fines Xanga for violating kids' privacy". NBCNews.com. NBCUniversal Media, LLC. Retrieved June 22, 2016.
 "Mobile Advertising Network InMobi Settles FTC Charges It Tracked Hundreds of Millions of Consumers' Locations Without Permission". Federal Trade Commission. June 22, 2016. Retrieved March 4, 2018.
 "Imbee.com Settles FTC Charges Social Networking Site for Kids Violated the Children's Online Privacy Protection Act; Settlement Includes $130,000 Civil Penalty". FTC Press Releases. Federal Trade Commission. January 30, 2008. Retrieved June 22, 2016.
 Engle, M.K. (July 12, 2011). "Kidswirl, LLC, FTC File No. 112-3034" (PDF). Federal Trade Commission. Retrieved June 22, 2016.
 "Operator of Social Networking Website for Kids Settles FTC Charges Site Collected Kids Personal Information Without Parental Consent". FTC Press Releases. Federal Trade Commission. November 8, 2011. Retrieved June 22, 2016.
 "Largest FTC COPPA settlement requires Musical.ly to change its tune". Federal Trade Commission. February 27, 2019. Retrieved February 27, 2019.
 Fingas, Jon (May 6, 2019). "App stores pull dating apps after FTC warning about underage users". Engadget. Retrieved June 19, 2019.
 Fung, Brian (September 4, 2019). "Google and FTC reach $170 million settlement over alleged YouTube violations of kids' privacy". CNN Business. Retrieved September 4, 2019.
 Boyd, Danah; Hargittai, Eszter; Schultz, Jason; Palfrey, John (October 31, 2011). "Why parents help their children lie to Facebook about age: Unintended consequences of the 'Children's Online Privacy Protection Act'". First Monday. doi:10.5210/fm.v16i11.3850. ISSN 1396-0466.
 "Guidelines for child-oriented content on YouTube". Believe Digital. Retrieved August 22, 2020.
 Kelly, Makena; Alexander, Julia (November 13, 2019). "YouTube's new kids' content system has creators scrambling". The Verge. Retrieved November 22, 2019.
 Important Update for All Creators: Complying with COPPA. YouTube. November 12, 2019. Archived from the original on December 11, 2021.
 "FTC Issues Orders to Nine Social Media and Video Streaming Services Seeking Data About How They Collect, Use, and Present Information". Federal Trade Commission. December 14, 2020. Retrieved March 26, 2022.
 Public Domain One or more of the preceding sentences incorporates text from this source, which is in the public domain: "YouTube channel owners: Is your content directed to children?". Federal Trade Commission. November 22, 2019. Retrieved March 15, 2022.
 Singer, Natasha (December 19, 2022). "Epic Games to Pay $520 Million Over Children's Privacy and Trickery Charges". The New York Times. ISSN 0362-4331. Retrieved December 19, 2022.
 Percival IV, L.C.; Johnson, E. (July 1, 2013). "New Children's Online Privacy Protection Act (COPPA) Rule Now In Effect". The National Law Review. Ifrah PLLC. Retrieved June 22, 2016.
 Larose, C.J.; Siripurapu, J.M. (June 28, 2013). "Guide to Compliance with the Amended Children's Online Privacy Protection Act (COPPA) Rule". The National Law Review. Ifrah PLLC. Retrieved June 22, 2016.
 Larose, C.J. (June 29, 2013). "Amended Children's Online Privacy Protection Act (COPPA) Rule Compliance Deadline Approaching". The National Law Review. Ifrah PLLC. Retrieved June 22, 2016.
 "Children's Online Privacy Protection Rule: Not Just for Kids' Sites". FTC Business Center. Federal Trade Commission. April 2013. Retrieved July 7, 2013.
 "Why does Microsoft charge me when I create an account for my child?". support.microsoft.com. Retrieved March 26, 2017.
 "FTC Strengthens Kids' Privacy, Gives Parents Greater Control Over Their Information By Amending Childrens Online Privacy Protection Rule". FTC Press Releases. Federal Trade Commission. December 19, 2012. Retrieved June 22, 2016.
 "FTC Grants Approval for New COPPA Verifiable Parental Consent Method". FTC Press Releases. Federal Trade Commission. November 19, 2015. Retrieved June 22, 2016.
 "International". Federal Trade Commission. March 1, 2013. Retrieved March 26, 2022.
 Tonsager, Lindsay (January 9, 2015). "FTC Warns Foreign Mobile-App Developer To Comply With COPPA". Inside Privacy. Retrieved July 21, 2019.
 "Largest FTC COPPA settlement requires Musical.ly to change its tune". Federal Trade Commission. February 27, 2019. Retrieved September 1, 2021.
 Matecki, L.A. (2010). "Update: COPPA is Ineffective Legislation! Next Steps for Protecting Youth Privacy Rights in the Social Networking Era". Journal of Lawn and Social Policy. 5 (2): 7. Archived from the original on June 29, 2016. Retrieved June 22, 2016.
 Magid, L. (August 4, 2012). "Unintended Consequences of FTC's New COPPA Children's Online Privacy Rules". The Huffington Post. TheHuffingtonPost.com, Inc. Retrieved June 22, 2016.
 "New Internet Privacy Rules Will Not Protect Kids". Archived from the original on August 3, 2017. Retrieved August 2, 2017.
 Magid, L. (August 29, 2014). "Magid: Protecting children online needs to allow for their right to free speech". The Mercury News. Digital First Media. Archived from the original on March 25, 2016. Retrieved June 22, 2016.
 Morris, J. (November 23, 2010). "Ask CDT: Answers on First Amendment Rights Online". CDT Blog. Center for Democracy and Technology. Retrieved June 22, 2016.
 Puckett, J.M. (May 14, 2013). "Insider insights on COPPA". Emoderation Blog. Emoderation Limited. Archived from the original on November 18, 2016. Retrieved June 22, 2016.
 Boyd, D.; Hargittai, E.; Schultz, J.; Palfrey, J. (November 7, 2011). "Why parents help their children lie to Facebook about age: Unintended consequences of the Children's Online Privacy Protection Act". First Monday. 16 (11). Retrieved June 22, 2016.
 Griggs, B. (November 1, 2011). "Parents help kids lie to get on Facebook, study finds". CNN.com. Turner Broadcasting System, Inc. Retrieved June 22, 2016.
 Perlroth, N. (June 17, 2012). "Verifying Ages Online Is a Daunting Task, Even for Experts". The New York Times. Archived from the original on January 31, 2018. Retrieved July 21, 2019.
 Kluver, C. (July 5, 2013). "Parental Notification, the FTC and Kids Apps: What's COPPA all about?". Digital Media Diet. Archived from the original on September 6, 2015. Retrieved June 22, 2016.
 Chaey, C. (June 24, 2013). "Snapchat Debuts SnapKidz, A Sext-Free App For Kids Under 13". Fast Company. Mansueto Ventures, LLC. Retrieved June 22, 2016.
 Kamenetz, A. (June 28, 2013). "How the New COPPA Requirements Are Bad for Businesses and Kids". Fast Company. Mansueto Ventures, LLC. Retrieved June 22, 2016.
 Davis, W. (September 25, 2012). "IAB: Proposed Children's Privacy Rules Undermine Business Model". Online Media Daily. MediaPost Communications. Retrieved June 22, 2016.
 "Apple, Netflix and YouTube among Streamers Flouting EU Privacy Law, Say New Complaints". Fortune. Retrieved September 8, 2019.
 Feiner, Lauren (September 4, 2019). "YouTube fine shows the US government is not serious about a Big Tech crackdown". CNBC. Retrieved September 8, 2019.
 Binder, Matt (September 4, 2019). "YouTube's $170 million fine isn't enough—and part of the FTC knows it". Mashable. Retrieved September 8, 2019.
 Hostetler, David R (2013). "Children's privacy in virtual K-12 education: virtual solutions of the amended Children's Online Privacy Protection Act (COPPA) rule". North Carolina Journal of Law & Technology. Online Ed 167.
 Lev-Ram, M. (May 20, 2011). "Zuckerberg: Kids under 13 should be allowed on Facebook". Fortune. Time, Inc. Retrieved June 22, 2016.
 "Keeping Your Kids Safe Online". NPR.
 Tagarth, Shaun (November 20, 2019). "All You Need To Know About COPPA On Youtube". WGN Radio.
 "YouTube's new kids' content system has creators scrambling". The Verge. November 13, 2019.
 Hart, Matthew (November 18, 2019). "YouTube's FTC-Mandated Rules for Kids Content Infuriate Creators". Nerdist.
 "YouTube channel owners: Is your content directed to children?". Federal Trade Commission. November 22, 2019.
 "COPPA: Everything Content Creators Need To Know". TheGamer. November 23, 2019.
 "Misinformed YouTubers Are Undermining the Fight for Children's Privacy Online". Slate Magazine. November 27, 2019.
 "COPPA Hasn't Fined ANYONE Yet? ($42,000)". YouTube. Deep Humor. July 13, 2020. Archived from the original on December 11, 2021.
 Kelly, Makena (January 9, 2020). "'Eraser button' for children's data gains support in the House". The Verge. Retrieved January 17, 2020.
 Jenner; Saunders, Block LLP-David P.; Martinez, Jessica A. (January 14, 2020). "New Bill Seeks to Update Children's Online Privacy Protections with the PROTECT Kids Act". Lexology. Retrieved January 17, 2020.
 Eggerton, John (May 23, 2018). "Kids Online 'Erase Button' Penciled In Once Again". Multichannel. Retrieved January 17, 2020.
External links
Children's Online Privacy Protection Act (COPPA) of 1998, via Federal Trade Commission
16 C.F.R. Part 312, the FTC's Children's Online Privacy Protection Rule, via Government Printing Office
Six Step Compliance Plan for Your Business via Federal Trade Commission, Business Center
Children's Privacy, via Federal Trade Commission
FTC FAQ on COPPA compliance, via Federal Trade Commission
Cybertelecom :: COPPA Information on COPPA regulatory developments
Authority control databases Edit this at Wikidata
Categories: Acts of the 105th United States CongressChild safetyChildren's rights legislationAmerican children's websitesInternet law in the United StatesPrivacy law in the United StatesUnited States federal communications legislationUnited States federal computing legislationUnited States federal privacy legislationAgeism
=======================================================

California Consumer Privacy Act

Article
Talk
Read
Edit
View history

Tools
From Wikipedia, the free encyclopedia
California Consumer Privacy Act

California State Legislature
Full name	California Consumer Privacy Act of 2018[1]
Introduced	January 3, 2018
Signed into law	June 28, 2018
Governor	Jerry Brown
Code	California Civil Code
Section	1798.100
Resolution	AB-375 (2017–2018 Session)
Website	Assembly Bill No. 375
Status: Current legislation
The California Consumer Privacy Act (CCPA) is a state statute intended to enhance privacy rights and consumer protection for residents of the state of California in the United States. The bill was passed by the California State Legislature and signed into law by the Governor of California, Jerry Brown, on June 28, 2018, to amend Part 4 of Division 3 of the California Civil Code.[2] Officially called AB-375, the act was introduced by Ed Chau, member of the California State Assembly, and State Senator Robert Hertzberg.[3][4]

Amendments to the CCPA, in the form of Senate Bill 1121, were passed on September 13, 2018.[5][6] Additional substantive amendments were signed into law on October 11, 2019.[7] The CCPA became effective on January 1, 2020.[8] In November 2020, California voters passed Proposition 24, also known as the California Privacy Rights Act, which amends and expands the CCPA.[9]

Intentions of the Act
The intentions of the Act are to provide California residents with the right to:

Know what personal data is being collected about them.
Know whether their personal data is sold or disclosed and to whom.
Say no to the sale of personal data.
Access their personal data.
Request a business to delete any personal information about a consumer collected from that consumer.[10]
Not be discriminated against for exercising their privacy rights.
Compliance
The CCPA applies to any business, including any for-profit entity that collects consumers' personal data, does business in California, and satisfies at least one of the following thresholds:

Has annual gross revenues in excess of $25 million;
Buys, receives, or sells the personal information of 100,000 or more consumers or households; or
Earns more than half of its annual revenue from selling consumers' personal information.[11][12]
Organizations are required to "implement and maintain reasonable security procedures and practices" in protecting consumer data.[13]

The businesses that the CCPA refers to do not need to be physically present in California. As long as the business is active in the state and meets the requirements, they are considered to be under the CCPA. This includes transactions done on the Internet. In comparison to other privacy laws like the GDPR, the CCPA lacks clarity about its geographic range.[14]

Responsibility and accountability
Implement processes to obtain parental or guardian consent for minors under 13 years and the affirmative consent of minors between 13 and 16 years to data sharing for purposes (Cal. Civ. Code § 1798.120(c)).
"Do Not Sell My Personal Information" link on the home page of the website of the business, that will direct users to a web page enabling them, or someone they authorize, to opt out of the sale of the resident's personal information (Cal. Civ. Code § 1798.135(a)(1)).[15]
Designate methods for submitting data access requests, including, at a minimum, a toll-free telephone number (Cal. Civ. Code § 1798.130(a)).[16]
Update privacy policies with newly required information, including a description of California residents' rights (Cal. Civ. Code § 1798.135(a)(2)).[17]
Avoid requesting opt-in consent for 12 months after a California resident opts out (Cal. Civ. Code § 1798.135(a)(5)).[18]
Sanctions and remedies
The following sanctions and remedies can be imposed:

Companies, activists, associations, and others can be authorized to exercise opt-out rights on behalf of California residents (Cal. Civ. Code § 1798.135(c).[5]
Companies that become victims of data theft or other data security breaches can be ordered in civil class action lawsuits to pay statutory damages between $100 and $750 per California resident and incident, or actual damages, whichever is greater, and any other relief a court deems proper, subject to an option of the California Attorney General's Office to prosecute the company instead of allowing civil suits to be brought against it (Cal. Civ. Code § 1798.150).[5]
A fine up to $7,500 for each intentional violation and $2,500 for each unintentional violation (Cal. Civ. Code § 1798.155).[5]
Privacy notices must be accessible and have alternative format access clearly called out.[19]
Liability may also apply in respect of businesses in overseas countries who ship items into California.[20]
The CCPA differs from the Virginia Consumer Data Protection Act in that the former provides a private right of action, whereas the latter is enforced by the Attorney General's office.[21]

Definition of personal data
CCPA defines personal information as information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked (directly or indirectly) with a particular consumer or household such as a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, social security number, driver's license number, license plate number, passport number, or other similar identifiers.[2]

An additional caveat identifies, relates to, describes, or is capable of being associated with, a particular individual, including, but not limited to, their name, signature, Social Security number, physical characteristics or description, address, telephone number, passport number, driver's license or state identification card number, insurance policy number, education, employment, employment history, bank account number, credit card number, debit card number, or any other financial information, medical information, or health insurance information.[22]

It does not consider Publicly Available Information as personal.[23]

Key differences between CCPA and the European Union's General Data Protection Regulation (GDPR) include the scope and territorial reach of each, definitions related to protected information, levels of specificity, and an opt-out right for sales of personal information.[24] CCPA differs in definition of personal information from GDPR as in some cases the CCPA only considers data that was provided by a consumer. The GDPR does not make that distinction and covers all personal data regardless of source. In the event of sensitive personal information, this does not apply if the information was manifestly made public by the data subject themselves, following the exception under Art.9(2),e). As such, the definition in GDPR is much broader than defined in the CCPA.[25][26][27]

Personal data can also include online or social media profile information. Personal data is not limited to a number or a physical document but can also be online identities, accounts, and other personal information.

History
The California Consumer Privacy Act of 2018 was originally proposed as a ballot proposition by a privacy group known as Californians for Consumer Privacy.[28] The California DOJ approved the initiative's official language on December 18, 2017, allowing the group to begin collecting signatures.[29] In June 2018, the proponents gathered enough signatures to qualify the CCPA initiative for the November 2018 election.[30] In California, the state legislature cannot repeal or amend a ballot proposition once it is passed by voters.[31] In response to the CCPA ballot proposition, state legislators negotiated with Californians for Consumer Privacy to pass a less restrictive version of the CCPA in exchange for the withdrawal of the ballot proposition.[32]

The CCPA was passed by the state legislature and signed by Gov. Brown on June 28, 2018; it became effective on January 1, 2020.[33][34] The act's effect was dependent upon the withdrawal of initiative 17–0039, the Consumer Right to Privacy Act.[35] Five amendments were enacted and signed by Gov. Newsom on October 11, 2019.[36] Notice of DOJ's proposed regulations was also published October 11 in the Z Register; As of January 10, 2020 the OAL had not yet filed the final regulations with the Secretary of State, as required for the regulations to become effective.[36][37]

The California Privacy Rights Act of 2020 proposed several changes to the CCPA.[38] The Act, also known as 2020 California Proposition 24, expands existing data privacy laws by allowing consumers greater control of their personal data and establishing the California Privacy Protection Agency.[39] It passed, with a majority of voters approving the measure.[40]

Exemptions
Personal Health Information[3]
Financial information
A big area of the CCPA exemption is the personal health information (PHI) that is gathered.[41] Rather than the data being treated with the CCPA guidelines in mind, it is expected for PHI to adhere to the Health Insurance Portability and Accountability Act, otherwise known as HIPAA.[41] If the business collecting the data is related to clinical trials, then it must adhere to the "Common Rule".[42]

As for the information that is gathered by financial institutions, the institutions follow the California Financial Information Privacy act or the Gramm-Leach-Bliley Act depending on the situation.[41][43]

See also
Consumer protection
Digital Privacy
General Data Protection Regulation
Information privacy
Privacy Policy
References
 "AB-375, Chau. Privacy: personal information: businesses". California State Legislature. Retrieved 19 November 2018.
 The California Consumer Privacy Act of 2018.
 Lapowsky, Issie (June 28, 2018). "California Unanimously Passes Historic Privacy Bill". Wired. Retrieved September 17, 2019.
 "Bill Text - AB-375 Privacy: personal information: businesses". Leginfo.legislature.ca.gov. Retrieved 27 November 2018.
 "Bill Text - SB-1121 California Consumer Privacy Act of 2018". leginfo.legislature.ca.gov. Retrieved 2019-01-30.
 "How the new California data privacy act could impact all organizations". Information Management. Archived from the original on 2019-01-31. Retrieved 2019-01-30.
 "Governor Newsom Issues Legislative Update 10.11.19". 12 October 2019. Retrieved 2019-11-08.
 "2019 is the Year of . . . CCPA? [Infographic]". The National Law Review. January 8, 2019. Retrieved 2019-01-30.
 "Move Over, CCPA: The California Privacy Rights Act Gets the Spotlight Now". news.bloomberglaw.com. Retrieved 2020-12-10.
 Senate Bill No. 1120, Chapter 735, Sec.2, 1798.105
 "California Consumer Privacy Act (CCPA) Fact Sheet" (PDF). State of California - Department of Justice - Office of the Attorney General. Retrieved 2020-03-25.
 "CCPA Guide: Are You Covered by the CCPA". JD Supra. Retrieved 2019-01-30.
 "TITLE 1.81.5. California Consumer Privacy Act of 2018 - CA Legislative Information".
 Illman, Erin; Temple, Paul (Winter 2020). "California Consumer Privacy Act: What Companies Need to Know". The Business Lawyer. 75 (1): 1637–1646. ProQuest 2350105509.
 "Control Your Personal Information | CA Consumer Privacy Act". caprivacy.org. Archived from the original on 2019-01-31. Retrieved 2019-01-30.
 Valetk, Harry A.; Hengesbaugh, Brian (December 18, 2018). "A Practical Guide to CCPA Readiness: Implementing Calif.'s New Privacy Law (Part 2)". Corporate Counsel. Retrieved 2019-01-30.
 "Today's Law As Amended". leginfo.legislature.ca.gov. Retrieved 2019-01-30.
 Captain, Sean (2018-07-02). "Here are 5 key details in California's new privacy law". Fast Company. Retrieved 2019-01-30.
 "Federal accessibility laws don't matter — California's accessibility laws do". Medium.com. Retrieved 12 November 2018.
 "How does the California Consumer Privacy Act apply to Australian businesses?". www.gladwinlegal.com.au. 12 August 2020. Retrieved 24 August 2020.
 Rippy, Sarah (March 3, 2021). "Virginia passes the Consumer Data Protection Act". International Association of Privacy Professionals. Retrieved March 8, 2023.
 TITLE 1.81. CUSTOMER RECORDS[1798.80 - 1798.84] (Law DIVISION 3. OBLIGATIONS [1427 - 3273] e). California State Legislature. January 1, 2010. Public Domain This article incorporates text from this source, which is in the public domain.
 Privacy: personal information: businesses (Assembly Bill 1798.140/(o)(2)). California State Legislature. June 28, 2018.
 "How to Prepare for the CCPA – Here Are the Resources You Need". CGOC The Council. 2019-10-06. Archived from the original on 2019-10-09. Retrieved 2019-10-15.
 Fielding, John (Feb 4, 2019). "Four differences between the GDPR and the CCPA". HelpNet Security.
 "How to Prepare for the CCPA – Here Are the Resources You Need". CGOC. 2019-10-08. Archived from the original on 2019-10-09. Retrieved 2019-10-08.
 Skiera, Bernd; Miller, Klaus, M.; Jin, Yuxi (2022). The Impact of the General Data Protection Regulation (GDPR) on the Online Advertising Market. La Vergne: Bernd Skiera. p. [page needed]. ISBN 978-3-9824173-3-2. OCLC 1301513718.
 Wakabayashi, Daisuke (14 May 2018). "Silicon Valley Faces Regulatory Fight on Its Home Turf". The New York Times.
 "Proposed Initiative Enters Circulation: Establishes New Consumer Privacy Rights; Expands Liability For Consumer Data Breaches" (Press release). California Secretary of State. 18 December 2017.
 "The California Privacy Rights Act Has Passed: What's in It?". JD Supra. Retrieved 2020-12-10.
 "Laws governing the initiative process in California". Ballotpedia. Retrieved 2020-12-10.
 "California lawmakers agree to new consumer privacy rules that would avert showdown on the November ballot". Los Angeles Times. 2018-06-22. Retrieved 2020-12-10.
 "California Unanimously Passes Historic Privacy Bill". Wired. ISSN 1059-1028. Retrieved 2020-12-10.
 Stephens, John (2 July 2019). "California Consumer Privacy Act". Business and Corporate Litigation Committee Newsletter. American Bar Association.
 Cohen, Rodgin; Evangelakos, John; Mousavi, Nader; Schwartz, Matthew; Friedlander, Nicole (23 July 2018). "Sullivan & Cromwell Discusses California Consumer Privacy Act of 2018". CLS Blue Sky Blog. Columbia Law School.
 Das, Anjali; Ferrari, Stefanie (3 December 2019). "California Consumer Privacy Act Effective January 1: Update". The National Law Review.
 Hutnik, Alysa Zeltzer; Townley, Katie; Khouryanna, DiPrima (23 October 2019). "CCPA Draft Regulations: What to Know About Timing and Process". Ad Law Access.
 "California Proposition 24, Consumer Personal Information Law and Agency Initiative (2020)". Ballotpedia. Retrieved 2020-10-25.
 "Text of Proposed Laws - Proposition 24" (PDF). California Secretary of State. Archived (PDF) from the original on 2020-10-18.
 Hooks, Chris Nichols, Kris. "What We Know About California Proposition Results". www.capradio.org. Retrieved 2020-12-08.
 "California Consumer Privacy Act FAQs for Covered Businesses". Jackson Lewis. 2019-10-10. Retrieved 2020-11-11.
 "The California Consumer Privacy Act" (PDF).
 "Codes Display Text". leginfo.legislature.ca.gov. Retrieved 2020-11-11.
Further reading
Hautala, Laura (December 25, 2019). "CCPA: Everything you need to know about California's new privacy law". CNET. Retrieved December 31, 2019.
Hautala, Laura (December 27, 2019). "New privacy laws like CCPA could be headed to your state". CNET. Retrieved December 31, 2019.
Lapowsky, Issie (June 28, 2018). "California Unanimously Passes Historic Privacy Bill". Wired. ISSN 1059-1028.
Lyons, Kim (December 31, 2019). "The CCPA goes into effect January 1 but still isn't quite finished". The Verge. Retrieved December 31, 2019.
Morrison, Sara (December 30, 2019). "California's new privacy law, explained". Vox. Retrieved December 31, 2019.
Stephens, John (July 2, 2019). "California Consumer Privacy Act". American Bar Association. Retrieved December 31, 2019.
External links
The Civil Code of the State of California
California Attorney General's Office - privacy laws
How to Read a Privacy Policy
What is California Consumer Privacy Act (CCPA)
Categories: California statutes2018 in American lawData laws of the AmericasInternet privacy legislationUnited States disability legislation
